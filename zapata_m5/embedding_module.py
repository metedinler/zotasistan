# AÅŸaÄŸÄ±da, tartÄ±ÅŸmalarÄ±mÄ±z ve yapÄ±lan gÃ¼ncellemeler doÄŸrultusunda oluÅŸturulmuÅŸ, final sÃ¼rÃ¼mÃ¼ olan 
# **`embedding_module.py`** modÃ¼lÃ¼nÃ¼ bulabilirsiniz. Bu modÃ¼l, metni belirli boyutlarda parÃ§alara ayÄ±rma, 
# OpenAI API kullanarak embedding oluÅŸturma ve bÃ¼yÃ¼k dosyalarÄ±n iÅŸlenmesi iÃ§in ek hata yÃ¶netimi (robust embedding) mekanizmasÄ±nÄ± iÃ§eriyor.


import os
import time
import numpy as np
from openai import OpenAI
from config_module import config
from robust_embedding_module import robust_embed_text

def split_text(text, chunk_size=256, method="words"):
    """
    ğŸ“Œ Metni belirlenen chunk_size'a gÃ¶re bÃ¶ler.
    
    Args:
        text (str): ParÃ§alanacak metin.
        chunk_size (int): Her parÃ§a iÃ§in maksimum kelime sayÄ±sÄ± (varsayÄ±lan: 256).
        method (str): BÃ¶lme yÃ¶ntemi; "words" kelime bazÄ±nda, "paragraphs" ise paragraf bazÄ±nda bÃ¶lme.
        
    Returns:
        list: ParÃ§alara ayrÄ±lmÄ±ÅŸ metin parÃ§alarÄ±nÄ±n listesi.
    """
    if method == "paragraphs":
        # Paragraf bazlÄ± bÃ¶lme: Ã‡ift yeni satÄ±r ile ayrÄ±lan paragraflar
        paragraphs = [para.strip() for para in text.split("\n\n") if para.strip()]
        return paragraphs
    else:
        words = text.split()
        return [" ".join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]

def embed_text(text, model="text-embedding-ada-002"):
    """
    ğŸ“Œ OpenAI API kullanarak verilen metin iÃ§in embedding oluÅŸturur.
    
    Args:
        text (str): Embedding oluÅŸturulacak metin.
        model (str): KullanÄ±lacak model (varsayÄ±lan: "text-embedding-ada-002").
        
    Returns:
        list veya None: OluÅŸturulan embedding vektÃ¶rÃ¼ (Ã¶rneÄŸin, 1536 boyutlu liste) veya hata durumunda None.
    """
    try:
        client_instance = OpenAI(api_key=config.OPENAI_API_KEY)
        response = client_instance.embeddings.create(
            input=text,
            model=model
        )
        config.logger.info(f"âœ… Embedding oluÅŸturuldu (model: {model})")
        return response.data[0].embedding
    except Exception as e:
        config.logger.error(f"âŒ OpenAI embedding hatasÄ± (model: {model}): {e}")
        return None

def process_large_text(text, pdf_id, chunk_size=10000):
    """
    ğŸ“Œ BÃ¼yÃ¼k metinleri, belirlenen chunk boyutuna gÃ¶re parÃ§alara ayÄ±rarak her bir parÃ§a iÃ§in embedding oluÅŸturur.
    Bu fonksiyon, robust_embed_text fonksiyonunu kullanarak her parÃ§a iÃ§in hata yÃ¶netimi ve yeniden deneme mekanizmasÄ±nÄ± uygular.
    
    Args:
        text (str): Ä°ÅŸlenecek bÃ¼yÃ¼k metin.
        pdf_id (str): PDF'nin temel ID'si (takip ve loglama iÃ§in).
        chunk_size (int): Her parÃ§a iÃ§in maksimum boyut (varsayÄ±lan: 10000 karakter).
        
    Returns:
        list: OluÅŸturulan embedding vektÃ¶rlerinin listesi.
    """
    chunks = split_text(text, chunk_size, method="words")
    embeddings = []
    total_chunks = len(chunks)
    for i, chunk in enumerate(chunks):
        emb = robust_embed_text(chunk, pdf_id, i, total_chunks)
        if emb is None:
            config.logger.error(f"âŒ Embedding baÅŸarÄ±sÄ±z: PDF {pdf_id}, Chunk {i}")
        else:
            embeddings.append(emb)
        time.sleep(1)  # API rate limit korumasÄ±
    return embeddings

# ### AÃ§Ä±klamalar

# - **split_text:**  
#   - Ä°ki yÃ¶ntemi destekler: "words" (varsayÄ±lan) kelime bazÄ±nda bÃ¶lme ve "paragraphs" yÃ¶nteminde Ã§ift yeni satÄ±r (\\n\\n) ile ayrÄ±lmÄ±ÅŸ paragraflara bÃ¶lme.
#   - Bu, hem kÄ±sa metinler hem de bÃ¼yÃ¼k dosyalar iÃ§in esnek bÃ¶lme imkanÄ± sunar.

# - **embed_text:**  
#   - OpenAI API kullanarak, verilen metin Ã¼zerinden embedding hesaplar.
#   - Hata durumlarÄ±nda try/except bloÄŸu ile hatalar loglanÄ±r ve fonksiyon None dÃ¶ndÃ¼rÃ¼r.

# - **process_large_text:**  
#   - BÃ¼yÃ¼k dosyalarÄ±n daha verimli iÅŸlenmesi iÃ§in metni belirlenen chunk boyutuna gÃ¶re bÃ¶ler.
#   - Her bir chunk iÃ§in robust embedding (retry/backoff mekanizmasÄ±) uygulanÄ±r.
#   - API Ã§aÄŸrÄ±larÄ± arasÄ±nda kÄ±sa bir bekleme (1 saniye) eklenerek rate limit korunur.

# Bu final sÃ¼rÃ¼mÃ¼, Ã¶nceki sÃ¼rÃ¼mlere gÃ¶re daha saÄŸlam hata yÃ¶netimi, esnek metin bÃ¶lme seÃ§enekleri 
# ve bÃ¼yÃ¼k dosya iÅŸleme desteÄŸi sunuyor. EÄŸer baÅŸka bir geliÅŸtirme veya ekleme talebiniz varsa lÃ¼tfen belirtin.

import os
import time
import numpy as np
from concurrent.futures import ThreadPoolExecutor, as_completed
from openai import OpenAI
from sentence_transformers import SentenceTransformer
from config_module import config
from alternative_embedding_module import get_sentence_transformer, embed_text_with_model, get_available_models

# VarsayÄ±lan ayarlar
DEFAULT_MODEL_PRIORITY = ["contriever_large", "specter_large", "all_mpnet", "paraphrase_mpnet"]
OPENAI_MODEL = "text-embedding-ada-002"
MAX_RETRIES = 3
BACKOFF_FACTOR = 1.5
RATE_LIMIT_DELAY = 1  # API rate limit korumasÄ± iÃ§in sabit gecikme

class EmbeddingManager:
    """
    ğŸ“Œ Embedding iÅŸlemlerini yÃ¶neten sÄ±nÄ±f.
    - OpenAI API ve alternatif embedding modellerini destekler.
    - Retry mekanizmasÄ± ve circuit breaker ile hata toleranslÄ±dÄ±r.
    - Birden fazla model desteÄŸi ve parÃ§alama Ã¶zelliÄŸi iÃ§erir.
    """

    def __init__(self):
        self.openai_client = OpenAI(api_key=config.OPENAI_API_KEY)
        self.model_priority = DEFAULT_MODEL_PRIORITY
        self.circuit_breaker = {}  # Circuit breaker durumunu takip etmek iÃ§in

    def split_text(self, text, chunk_size=256, method="words"):
        """
        ğŸ“Œ Metni belirlenen chunk_size'a gÃ¶re bÃ¶ler.
        
        Args:
            text (str): ParÃ§alanacak metin.
            chunk_size (int): Her parÃ§a iÃ§in maksimum kelime sayÄ±sÄ± (varsayÄ±lan: 256).
            method (str): BÃ¶lme yÃ¶ntemi; "words" kelime bazÄ±nda, "paragraphs" ise paragraf bazÄ±nda bÃ¶lme.
            
        Returns:
            list: ParÃ§alara ayrÄ±lmÄ±ÅŸ metin parÃ§alarÄ±nÄ±n listesi.
        """
        if method == "paragraphs":
            paragraphs = [para.strip() for para in text.split("\n\n") if para.strip()]
            return paragraphs
        else:
            words = text.split()
            return [" ".join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]

    def robust_embed_text(self, text, pdf_id, chunk_index, total_chunks, model_priority=None, max_retries=MAX_RETRIES, backoff_factor=BACKOFF_FACTOR):
        """
        ğŸ“Œ Verilen metni embedding oluÅŸtururken hata toleranslÄ± bir mekanizma kullanÄ±r.
        
        Args:
            text (str): Embedding oluÅŸturulacak metin.
            pdf_id (str): PDF dosya kimliÄŸi.
            chunk_index (int): Ä°ÅŸlenen metin parÃ§asÄ±nÄ±n sÄ±rasÄ±.
            total_chunks (int): Toplam metin parÃ§a sayÄ±sÄ±.
            model_priority (list, optional): KullanÄ±lacak model sÄ±rasÄ±. VarsayÄ±lan olarak `DEFAULT_MODEL_PRIORITY`.
            max_retries (int): Her model iÃ§in en fazla kaÃ§ kez tekrar deneneceÄŸi.
            backoff_factor (float): Hata alÄ±ndÄ±ÄŸÄ±nda bekleme sÃ¼resini artÄ±ran katsayÄ±.
        
        Returns:
            dict: BaÅŸarÄ±lÄ± embedding vektÃ¶rÃ¼ ve kullanÄ±lan model bilgisi.
        """
        if model_priority is None:
            model_priority = self.model_priority

        # Ã–ncelikle OpenAI API ile embedding oluÅŸturmaya Ã§alÄ±ÅŸ
        if OPENAI_MODEL not in self.circuit_breaker or not self.circuit_breaker[OPENAI_MODEL]:
            try:
                response = self.openai_client.embeddings.create(
                    input=text,
                    model=OPENAI_MODEL
                )
                return {"embedding": response.data[0].embedding, "model": OPENAI_MODEL}
            except Exception as e:
                config.logger.warning(f"âš ï¸ OpenAI modeli baÅŸarÄ±sÄ±z ({OPENAI_MODEL}), alternatif modellere geÃ§iliyor. Hata: {e}")
                self.circuit_breaker[OPENAI_MODEL] = True  # Circuit breaker devreye girer

        # OpenAI baÅŸarÄ±sÄ±z olduysa, alternatif modellere geÃ§
        for model_key in model_priority:
            if model_key in self.circuit_breaker and self.circuit_breaker[model_key]:
                continue  # Circuit breaker aÃ§Ä±k olan modeller atlanÄ±r

            for attempt in range(1, max_retries + 1):
                try:
                    embedding = embed_text_with_model(text, model_key)
                    if embedding:
                        return {"embedding": embedding, "model": model_key}
                except Exception as e:
                    wait_time = backoff_factor ** attempt
                    config.logger.error(f"âŒ {model_key} ile embedding baÅŸarÄ±sÄ±z! ({attempt}/{max_retries}) Hata: {e}")
                    time.sleep(wait_time)  # Backoff delay
                    if attempt == max_retries:
                        self.circuit_breaker[model_key] = True  # Circuit breaker devreye girer

        # TÃ¼m denemeler baÅŸarÄ±sÄ±z olursa None dÃ¶ndÃ¼r
        config.logger.critical(f"ğŸš¨ Embedding iÅŸlemi tamamen baÅŸarÄ±sÄ±z oldu! (PDF: {pdf_id}, Chunk: {chunk_index}/{total_chunks})")
        return {"embedding": None, "model": "failed"}

    def process_large_text(self, text, pdf_id, chunk_size=256, method="words"):
        """
        ğŸ“Œ BÃ¼yÃ¼k metinleri parÃ§alara ayÄ±rarak her bir parÃ§a iÃ§in embedding oluÅŸturur.
        
        Args:
            text (str): Ä°ÅŸlenecek bÃ¼yÃ¼k metin.
            pdf_id (str): PDF dosya kimliÄŸi.
            chunk_size (int): Her parÃ§a iÃ§in maksimum kelime sayÄ±sÄ± (varsayÄ±lan: 256).
            method (str): BÃ¶lme yÃ¶ntemi; "words" kelime bazÄ±nda, "paragraphs" ise paragraf bazÄ±nda bÃ¶lme.
        
        Returns:
            list: OluÅŸturulan embedding vektÃ¶rlerinin listesi.
        """
        chunks = self.split_text(text, chunk_size, method)
        embeddings = []
        total_chunks = len(chunks)

        with ThreadPoolExecutor() as executor:
            futures = {
                executor.submit(
                    self.robust_embed_text,
                    chunk,
                    pdf_id,
                    i,
                    total_chunks
                ): chunk for i, chunk in enumerate(chunks)
            }
            for future in as_completed(futures):
                result = future.result()
                embeddings.append(result)
                time.sleep(RATE_LIMIT_DELAY)  # API rate limit korumasÄ±

        return embeddings

    def save_embeddings(self, embeddings, output_path):
        """
        ğŸ“Œ Embedding verilerini JSON formatÄ±nda kaydeder.
        
        Args:
            embeddings (list): Kaydedilecek embedding verileri.
            output_path (str): Kaydedilecek dosyanÄ±n yolu.
        """
        try:
            with open(output_path, "w", encoding="utf-8") as f:
                json.dump(embeddings, f, ensure_ascii=False, indent=4)
            config.logger.info(f"âœ… Embedding verileri baÅŸarÄ±yla kaydedildi: {output_path}")
        except Exception as e:
            config.logger.error(f"âŒ Embedding verileri kaydedilemedi: {e}")


# Bu ModÃ¼lÃ¼n Ã–zellikleri
# API Entegrasyonu:
# OpenAI API'si (text-embedding-ada-002) Ã¶ncelikli olarak kullanÄ±lÄ±r.
# Alternatif embedding modelleri (sentence-transformers vb.) desteklenir.
# ParÃ§alama DesteÄŸi:
# Metinler kelime veya paragraf bazÄ±nda parÃ§alanabilir.
# BÃ¼yÃ¼k metinler iÃ§in parÃ§alama ve embedding oluÅŸturma desteÄŸi vardÄ±r.
# Retry MekanizmasÄ±:
# Her model iÃ§in belirlenen sayÄ±da yeniden deneme yapÄ±lÄ±r.
# Hata durumunda exponential backoff uygulanÄ±r.
# Circuit Breaker:
# BaÅŸarÄ±sÄ±z olan modeller devre dÄ±ÅŸÄ± bÄ±rakÄ±lÄ±r ve tekrar denenmez.
# Birden Fazla Model DesteÄŸi:
# alternative_embedding_module.py'den gelen alternatif modeller desteklenir.
# Model sÄ±rasÄ± (model_priority) deÄŸiÅŸtirilebilir.
# API Rate Limit KorumasÄ±:
# Sabit time.sleep(1) ile API rate limit korumasÄ± saÄŸlanÄ±r.
# Loglama ve Hata YÃ¶netimi:
# GÃ¼Ã§lÃ¼ loglama sistemi ile tÃ¼m iÅŸlemler izlenir.
# Hatalar detaylÄ± olarak loglanÄ±r.
# KullanÄ±m Ã–rnekleri
# 1. BÃ¼yÃ¼k Metin Ä°Ã§in Embedding OluÅŸturma:

# embedding_manager = EmbeddingManager()
# large_text = "Bu Ã§ok bÃ¼yÃ¼k bir metin..."
# pdf_id = "ABCD1234"
# embeddings = embedding_manager.process_large_text(large_text, pdf_id, chunk_size=256, method="words")
# embedding_manager.save_embeddings(embeddings, "embeddings.json")
# 2. Tek Bir Metin Ä°Ã§in Embedding OluÅŸturma:

# embedding_manager = EmbeddingManager()
# text = "Bu bir Ã¶rnek metin."
# pdf_id = "ABCD1234"
# result = embedding_manager.robust_embed_text(text, pdf_id, chunk_index=0, total_chunks=1)
# print(result)