import os
import logging
from pathlib import Path
from dotenv import load_dotenv

# .env dosyasÄ±nÄ± belirtilen yoldan yÃ¼kle
load_dotenv("C:/Users/mete/Zotero/zotasistan/.env")

class Config:
    """
    Uygulama genelinde kullanÄ±lacak yapÄ±landÄ±rma ayarlarÄ±nÄ± yÃ¶netir.
    - Ortam deÄŸiÅŸkenlerini (.env) yÃ¼kler.
    - Gerekli dizinleri oluÅŸturur.
    - Merkezi loglama sistemini yapÄ±landÄ±rÄ±r.
    """
    # Base directory (proje kÃ¶k dizini olarak kullanÄ±labilir)
    BASE_DIR = Path(__file__).resolve().parent.parent

    # Dizin YapÄ±larÄ± (VarsayÄ±lan deÄŸerler .env Ã¼zerinden de ayarlanabilir)
    STORAGE_DIR = Path(os.getenv("STORAGE_DIR", BASE_DIR / "storage"))
    SUCCESS_DIR = Path(os.getenv("SUCCESS_DIR", BASE_DIR / "processed" / "success"))
    CITATIONS_DIR = Path(os.getenv("CITATIONS_DIR", BASE_DIR / "processed" / "citations"))
    TABLES_DIR = Path(os.getenv("TABLES_DIR", BASE_DIR / "processed" / "tables"))
    CLEAN_TEXT_DIR = Path(os.getenv("TEMIZMETIN_DIR", BASE_DIR / "processed" / "clean_text"))
    EMBEDDINGS_DIR = Path(os.getenv("EMBEDDING_PARCA_DIZIN", BASE_DIR / "processed" / "embeddings"))
    TEMP_DIR = Path(os.getenv("TEMP_DIR", BASE_DIR / "temp"))
    LOG_DIR = Path(os.getenv("LOG_DIR", BASE_DIR / "logs"))

    # Gerekli dizinlerin oluÅŸturulmasÄ±
    for directory in [STORAGE_DIR, SUCCESS_DIR, CITATIONS_DIR, TABLES_DIR, CLEAN_TEXT_DIR, EMBEDDINGS_DIR, TEMP_DIR, LOG_DIR]:
        directory.mkdir(parents=True, exist_ok=True)

    # Log DosyasÄ±
    LOG_FILE = LOG_DIR / "app.log"

    # Loglama yapÄ±landÄ±rmasÄ±
    logging.basicConfig(
        filename=LOG_FILE,
        level=logging.DEBUG,  # Ä°steÄŸe baÄŸlÄ± olarak os.getenv("LOG_LEVEL", "DEBUG") kullanÄ±labilir
        format='%(asctime)s - %(levelname)s - %(message)s',
        encoding='utf-8'
    )
    logger = logging.getLogger(__name__)

    # PDF Metin Ã‡Ä±karma YÃ¶ntemi (.envâ€™den okunur)
    PDF_TEXT_EXTRACTION_METHOD = os.getenv("PDF_TEXT_EXTRACTION_METHOD", "pdfplumber").lower()

    # Zotero API AyarlarÄ±
    ZOTERO_USER_ID = os.getenv("ZOTERO_USER_ID")
    ZOTERO_API_KEY = os.getenv("ZOTERO_API_KEY")
    ZOTERO_LIBRARY_TYPE = os.getenv("ZOTERO_LIBRARY_TYPE", "user")  # user veya group

    # OpenAI API ve Embedding AyarlarÄ±
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
    EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL", "text-embedding-ada-002")
    
    # Alternatif Embedding Modelleri (env Ã¼zerinden ayarlanabilir)
    EMBEDDING_MODELS = {
        "contriever_large": os.getenv("CONTRIEVER_LARGE_MODEL", "facebook/contriever-large"),
        "specter_large": os.getenv("SPECTER_LARGE_MODEL", "allenai-specter-large"),
        "stsb_roberta_large": os.getenv("STSB_ROBERTA_LARGE_MODEL", "stsb-roberta-large"),
        "labse": os.getenv("LABSE_MODEL", "LaBSE"),
        "universal_sentence_encoder": os.getenv("USE_MODEL", "universal-sentence-encoder"),
    }

    # Chunk ve BÃ¼yÃ¼k Dosya Ä°ÅŸleme AyarlarÄ±
    CHUNK_SIZE = int(os.getenv("CHUNK_SIZE", 256))
    LARGE_FILE_SPLIT_SIZE = int(os.getenv("LARGE_FILE_SPLIT_SIZE", 10000))

    # NLP ve Regex AyarlarÄ± (Bilimsel bÃ¶lÃ¼mler iÃ§in)
    REGEX_SECTION_PATTERNS = {
        "Abstract": r"(?:^|\n)(Abstract|Ã–zet)(?::)?\s*\n",
        "Introduction": r"(?:^|\n)(Introduction|GiriÅŸ)(?::)?\s*\n",
        "Methods": r"(?:^|\n)(Methods|Materials and Methods|YÃ¶ntemler|Metot)(?::)?\s*\n",
        "Results": r"(?:^|\n)(Results|Bulgular)(?::)?\s*\n",
        "Discussion": r"(?:^|\n)(Discussion|TartÄ±ÅŸma)(?::)?\s*\n",
        "Conclusion": r"(?:^|\n)(Conclusion|SonuÃ§)(?::)?\s*\n"
    }

    # Tablo tespiti iÃ§in regex desenleri
    TABLE_DETECTION_PATTERNS = [
        r"(?:^|\n)(Tablo|Table|Ã‡izelge|Chart)\s*\d+",
        r"(?:^|\n)(Åekil|Figure|Fig)\s*\d+"
    ]

    # Paralel Ä°ÅŸlem ve Hata YÃ¶netimi AyarlarÄ±
    MAX_RETRIES = int(os.getenv("MAX_RETRIES", 3))
    BACKOFF_FACTOR = float(os.getenv("BACKOFF_FACTOR", 1.0))

    # GUI AyarlarÄ±
    GUI_THEME = os.getenv("GUI_THEME", "light")

config = Config()
#---------------------------------------config modulu sonu------------------------------------------------
import re
import requests
from config_module import config

class ZoteroEntegratoru:
    """
    Zotero API entegrasyonunu yÃ¶neten sÄ±nÄ±f.
    
    Bu sÄ±nÄ±f:
    - Zotero API'si aracÄ±lÄ±ÄŸÄ±yla belirtilen item key'e gÃ¶re bibliyometrik verileri Ã§eker.
    - Verilen referans listesini analiz edip, basit regex kullanarak yazar isimlerini Ã§Ä±karmayÄ± saÄŸlar.
    """
    def __init__(self):
        self.base_url = f"https://api.zotero.org/users/{config.ZOTERO_USER_ID}/items"
        self.headers = {"Zotero-API-Key": config.ZOTERO_API_KEY}
    
    def meta_veri_al(self, item_key):
        """
        Belirtilen Zotero item key'e gÃ¶re bibliyometrik verileri Ã§eker.
        
        Args:
            item_key (str): Zotero item key.
            
        Returns:
            dict or None: Ã‡ekilen bibliyometrik veriler; hata durumunda None.
        """
        url = f"{self.base_url}/{item_key}"
        try:
            response = requests.get(url, headers=self.headers, timeout=10)
            if response.status_code == 200:
                config.logger.info(f"Zotero API'den veri Ã§ekildi: {item_key}")
                return response.json()
            else:
                config.logger.error(f"Zotero API hatasÄ± {response.status_code}: {url}")
                return None
        except Exception as e:
            config.logger.error(f"Zotero API isteÄŸinde hata: {e}")
            return None

    def referanslari_analiz_et(self, referans_listesi):
        """
        Verilen referans listesini analiz eder ve her referans iÃ§in yazar ismini Ã§Ä±karÄ±r.
        
        Args:
            referans_listesi (list): KaynakÃ§a referans metinlerinin listesi.
            
        Returns:
            list: Her referans iÃ§in analiz sonucu iÃ§eren sÃ¶zlÃ¼klerin listesi.
                  Ã–rnek: [{"orijinal": "Smith, 2020. Title...", "yazar": "Smith"}, ...]
        """
        analiz_sonuc = []
        try:
            for referans in referans_listesi:
                # Basit regex: Referans metninin baÅŸÄ±ndaki ilk kelimeyi yazar olarak kabul ediyoruz.
                match = re.search(r'^([\w\.\-]+)', referans)
                yazar = match.group(1) if match else "Bilinmeyen"
                analiz_sonuc.append({
                    "orijinal": referans,
                    "yazar": yazar
                })
            config.logger.info("Referans analizi tamamlandÄ±.")
            return analiz_sonuc
        except Exception as e:
            config.logger.error(f"Referans analizi hatasÄ±: {e}")
            return []
#---------------------------------------zotero modulu sonu------------------------------------------------
import os
import re
from config_module import config

def extract_text_from_pdf(pdf_path, method=None):
    """
    PDF'den ham metni Ã§Ä±karÄ±r.
    EÄŸer method parametresi verilmezse, .env dosyasÄ±ndan "PDF_TEXT_EXTRACTION_METHOD" okunur (varsayÄ±lan: "pdfplumber").
    EÄŸer pdfplumber ile metin Ã§Ä±karma hatasÄ± alÄ±nÄ±rsa, otomatik olarak pdfminer yÃ¶ntemi devreye girer.
    
    Args:
        pdf_path (str or Path): PDF dosyasÄ±nÄ±n yolu.
        method (str, optional): KullanÄ±lacak metin Ã§Ä±karma yÃ¶ntemi ("pdfplumber" veya "pdfminer").
        
    Returns:
        str veya None: Ã‡Ä±karÄ±lan metin; hata durumunda None.
    """
    if method is None:
        method = os.getenv("PDF_TEXT_EXTRACTION_METHOD", "pdfplumber").lower()

    text = None
    if method == "pdfplumber":
        try:
            import pdfplumber
            with pdfplumber.open(pdf_path) as pdf:
                pages_text = []
                for page in pdf.pages:
                    page_text = page.extract_text()
                    if page_text:
                        pages_text.append(page_text)
                text = "\n".join(pages_text)
            config.logger.info(f"âœ… pdfplumber ile metin Ã§Ä±karÄ±ldÄ±: {pdf_path}")
        except Exception as e:
            config.logger.error(f"âŒ pdfplumber ile metin Ã§Ä±karma hatasÄ±: {e}. pdfminer deneniyor.")
            return extract_text_from_pdf(pdf_path, method="pdfminer")
    elif method == "pdfminer":
        try:
            from pdfminer.high_level import extract_text
            text = extract_text(pdf_path)
            config.logger.info(f"âœ… pdfminer ile metin Ã§Ä±karÄ±ldÄ±: {pdf_path}")
        except Exception as e:
            config.logger.error(f"âŒ pdfminer ile metin Ã§Ä±karma hatasÄ±: {e}")
    else:
        config.logger.error("GeÃ§ersiz method belirtildi. 'pdfplumber' veya 'pdfminer' kullanÄ±labilir.")
    return text

def detect_columns(text, min_gap=4):
    """
    Metindeki sÃ¼tun yapÄ±sÄ±nÄ± tespit eder.
    Belirli bir boÅŸluk sayÄ±sÄ±na gÃ¶re (min_gap) metnin sÃ¼tunlu olup olmadÄ±ÄŸÄ±nÄ± belirler.
    
    Args:
        text (str): Ä°ÅŸlenecek metin.
        min_gap (int): Bir satÄ±rda sÃ¼tunlarÄ± ayÄ±rmak iÃ§in gereken minimum boÅŸluk sayÄ±sÄ±.
    
    Returns:
        dict: Ã–rnek: {'sutunlu': True} veya {'sutunlu': False}
    """
    lines = text.split('\n')
    column_line_count = sum(1 for line in lines if re.search(r' {' + str(min_gap) + r',}', line))
    return {'sutunlu': column_line_count > len(lines) * 0.2}

def map_scientific_sections_extended(text):
    """
    Bilimsel dokÃ¼manlarÄ±n bÃ¶lÃ¼mlerini haritalar.
    Ã–rneÄŸin: Abstract, Introduction, Methods, Results, Discussion, Conclusion,
    Ä°Ã§indekiler, Tablolar, Ã‡izelgeler, Resimler/FigÃ¼rler, Ä°ndeks.
    
    Args:
        text (str): Ä°ÅŸlenecek ham metin.
        
    Returns:
        dict: HaritalanmÄ±ÅŸ bÃ¶lÃ¼mler; her bÃ¶lÃ¼m iÃ§in "start", "end" ve "content" bilgileri.
    """
    section_patterns = {
        "Abstract": r"(?:^|\n)(Abstract|Ã–zet)(?::)?\s*\n",
        "Introduction": r"(?:^|\n)(Introduction|GiriÅŸ)(?::)?\s*\n",
        "Methods": r"(?:^|\n)(Methods|Materials and Methods|YÃ¶ntemler|Metot)(?::)?\s*\n",
        "Results": r"(?:^|\n)(Results|Bulgular)(?::)?\s*\n",
        "Discussion": r"(?:^|\n)(Discussion|TartÄ±ÅŸma)(?::)?\s*\n",
        "Conclusion": r"(?:^|\n)(Conclusion|SonuÃ§)(?::)?\s*\n"
    }
    additional_patterns = {
        "Ä°Ã§indekiler": r"(?:^|\n)(Ä°Ã§indekiler)(?::)?\s*\n",
        "Tablolar": r"(?:^|\n)(Tablolar|Tables)(?::)?\s*\n",
        "Ã‡izelgeler": r"(?:^|\n)(Ã‡izelgeler|Charts)(?::)?\s*\n",
        "Resimler/FigÃ¼rler": r"(?:^|\n)(Resimler|Figures)(?::)?\s*\n",
        "Ä°ndeks": r"(?:^|\n)(Ä°ndeks|Index)(?::)?\s*\n"
    }
    sections_map = {}
    # Her bÃ¶lÃ¼m iÃ§in ilk eÅŸleÅŸmenin baÅŸlangÄ±Ã§ indeksini alÄ±yoruz.
    for section, pattern in {**section_patterns, **additional_patterns}.items():
        matches = list(re.finditer(pattern, text, flags=re.IGNORECASE))
        sections_map[section] = matches[0].start() if matches else None

    # Sadece bulunan bÃ¶lÃ¼mleri ayÄ±kla
    detected_sections = {sec: pos for sec, pos in sections_map.items() if pos is not None}
    sorted_sections = sorted(detected_sections.items(), key=lambda x: x[1])
    mapped_sections = {}
    for i, (section, start_idx) in enumerate(sorted_sections):
        end_idx = sorted_sections[i + 1][1] if i + 1 < len(sorted_sections) else len(text)
        content = text[start_idx:end_idx].strip()
        mapped_sections[section] = {"start": start_idx, "end": end_idx, "content": content}
    # Ek olarak, sÃ¼tun yapÄ±sÄ± bilgisi ekleyelim
    column_info = detect_columns(text)
    mapped_sections["Column Structure"] = column_info

    # EÄŸer bazÄ± bÃ¶lÃ¼mler bulunamazsa, onlarÄ± None olarak ekleyelim
    for sec in list(section_patterns.keys()) + list(additional_patterns.keys()):
        if sec not in mapped_sections:
            mapped_sections[sec] = None

    return mapped_sections

def map_pdf_before_extraction(pdf_path, method='pdfplumber'):
    """
    PDF'den metin Ã§Ä±karÄ±lmadan Ã¶nce yapÄ±sal analiz yapar ve bilimsel bÃ¶lÃ¼mleri haritalar.
    Ä°lk olarak, extract_text_from_pdf Ã§aÄŸrÄ±lÄ±r; ardÄ±ndan elde edilen metin Ã¼zerinden
    map_scientific_sections_extended fonksiyonu ile bÃ¶lÃ¼mler tespit edilir.
    
    Args:
        pdf_path (str or Path): PDF dosyasÄ±nÄ±n yolu.
        method (str): KullanÄ±lacak metot ("pdfplumber" veya "pdfminer"). VarsayÄ±lan: "pdfplumber".
    
    Returns:
        dict veya None: HaritalanmÄ±ÅŸ bÃ¶lÃ¼m bilgileri; metin Ã§Ä±karÄ±lamazsa None.
    """
    text = extract_text_from_pdf(pdf_path, method=method)
    if not text:
        config.logger.error("PDF'den metin Ã§Ä±karÄ±lamadÄ±; haritalama yapÄ±lamÄ±yor.")
        return None
    return map_scientific_sections_extended(text)

def reflow_columns(text):
    """
    SÃ¼tunlu metni tek akÄ±ÅŸa dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.
    - HTML/Markdown etiketlerini temizler.
    - Sayfa baÅŸÄ±/sonu bilgilerini (Ã¶rn. "Page 1", "Sayfa 1") kaldÄ±rÄ±r.
    - Fazla boÅŸluklarÄ± tek boÅŸluk yapar.
    - KÄ±rpÄ±lmÄ±ÅŸ kelimelerdeki tire iÅŸaretlerini kaldÄ±rÄ±r.
    
    Args:
        text (str): Ä°ÅŸlenecek metin.
    
    Returns:
        str: TemizlenmiÅŸ, tek akÄ±ÅŸa dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lmÃ¼ÅŸ metin.
    """
    # HTML etiketlerini kaldÄ±r
    text = re.sub(r"<[^>]+>", " ", text)
    # Markdown link yapÄ±larÄ±nÄ± kaldÄ±r: [Link](url)
    text = re.sub(r"\[[^\]]+\]\([^)]+\)", " ", text)
    # Sayfa bilgilerini temizle
    text = re.sub(r"(Page|Sayfa)\s*\d+", " ", text, flags=re.IGNORECASE)
    # SatÄ±r sonlarÄ±nÄ± boÅŸlukla deÄŸiÅŸtir
    text = re.sub(r"\n", " ", text)
    # Fazla boÅŸluklarÄ± tek boÅŸluk yap
    text = re.sub(r"\s{2,}", " ", text)
    # KÄ±rpÄ±lmÄ±ÅŸ kelimelerdeki tireyi kaldÄ±r: "infor- mation" -> "information"
    text = re.sub(r"(\w+)-\s+(\w+)", r"\1\2", text)
    return text.strip()
#---------------------------------------pdf_processing modulu sonu------------------------------------------------
import os
import time
import numpy as np
from concurrent.futures import ThreadPoolExecutor, as_completed
from openai import OpenAI
from sentence_transformers import SentenceTransformer
from config_module import config
from alternative_embedding_module import get_sentence_transformer, embed_text_with_model, get_available_models

# VarsayÄ±lan ayarlar
DEFAULT_MODEL_PRIORITY = ["contriever_large", "specter_large", "all_mpnet", "paraphrase_mpnet"]
OPENAI_MODEL = "text-embedding-ada-002"
MAX_RETRIES = 3
BACKOFF_FACTOR = 1.5
RATE_LIMIT_DELAY = 1  # API rate limit korumasÄ± iÃ§in sabit gecikme

class EmbeddingManager:
    """
    ğŸ“Œ Embedding iÅŸlemlerini yÃ¶neten sÄ±nÄ±f.
    - OpenAI API ve alternatif embedding modellerini destekler.
    - Retry mekanizmasÄ± ve circuit breaker ile hata toleranslÄ±dÄ±r.
    - Birden fazla model desteÄŸi ve parÃ§alama Ã¶zelliÄŸi iÃ§erir.
    """

    def __init__(self):
        self.openai_client = OpenAI(api_key=config.OPENAI_API_KEY)
        self.model_priority = DEFAULT_MODEL_PRIORITY
        self.circuit_breaker = {}  # Circuit breaker durumunu takip etmek iÃ§in

    def split_text(self, text, chunk_size=256, method="words"):
        """
        ğŸ“Œ Metni belirlenen chunk_size'a gÃ¶re bÃ¶ler.
        
        Args:
            text (str): ParÃ§alanacak metin.
            chunk_size (int): Her parÃ§a iÃ§in maksimum kelime sayÄ±sÄ± (varsayÄ±lan: 256).
            method (str): BÃ¶lme yÃ¶ntemi; "words" kelime bazÄ±nda, "paragraphs" ise paragraf bazÄ±nda bÃ¶lme.
            
        Returns:
            list: ParÃ§alara ayrÄ±lmÄ±ÅŸ metin parÃ§alarÄ±nÄ±n listesi.
        """
        if method == "paragraphs":
            paragraphs = [para.strip() for para in text.split("\n\n") if para.strip()]
            return paragraphs
        else:
            words = text.split()
            return [" ".join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]

    def robust_embed_text(self, text, pdf_id, chunk_index, total_chunks, model_priority=None, max_retries=MAX_RETRIES, backoff_factor=BACKOFF_FACTOR):
        """
        ğŸ“Œ Verilen metni embedding oluÅŸtururken hata toleranslÄ± bir mekanizma kullanÄ±r.
        
        Args:
            text (str): Embedding oluÅŸturulacak metin.
            pdf_id (str): PDF dosya kimliÄŸi.
            chunk_index (int): Ä°ÅŸlenen metin parÃ§asÄ±nÄ±n sÄ±rasÄ±.
            total_chunks (int): Toplam metin parÃ§a sayÄ±sÄ±.
            model_priority (list, optional): KullanÄ±lacak model sÄ±rasÄ±. VarsayÄ±lan olarak `DEFAULT_MODEL_PRIORITY`.
            max_retries (int): Her model iÃ§in en fazla kaÃ§ kez tekrar deneneceÄŸi.
            backoff_factor (float): Hata alÄ±ndÄ±ÄŸÄ±nda bekleme sÃ¼resini artÄ±ran katsayÄ±.
        
        Returns:
            dict: BaÅŸarÄ±lÄ± embedding vektÃ¶rÃ¼ ve kullanÄ±lan model bilgisi.
        """
        if model_priority is None:
            model_priority = self.model_priority

        # Ã–ncelikle OpenAI API ile embedding oluÅŸturmaya Ã§alÄ±ÅŸ
        if OPENAI_MODEL not in self.circuit_breaker or not self.circuit_breaker[OPENAI_MODEL]:
            try:
                response = self.openai_client.embeddings.create(
                    input=text,
                    model=OPENAI_MODEL
                )
                return {"embedding": response.data[0].embedding, "model": OPENAI_MODEL}
            except Exception as e:
                config.logger.warning(f"âš ï¸ OpenAI modeli baÅŸarÄ±sÄ±z ({OPENAI_MODEL}), alternatif modellere geÃ§iliyor. Hata: {e}")
                self.circuit_breaker[OPENAI_MODEL] = True  # Circuit breaker devreye girer

        # OpenAI baÅŸarÄ±sÄ±z olduysa, alternatif modellere geÃ§
        for model_key in model_priority:
            if model_key in self.circuit_breaker and self.circuit_breaker[model_key]:
                continue  # Circuit breaker aÃ§Ä±k olan modeller atlanÄ±r

            for attempt in range(1, max_retries + 1):
                try:
                    embedding = embed_text_with_model(text, model_key)
                    if embedding:
                        return {"embedding": embedding, "model": model_key}
                except Exception as e:
                    wait_time = backoff_factor ** attempt
                    config.logger.error(f"âŒ {model_key} ile embedding baÅŸarÄ±sÄ±z! ({attempt}/{max_retries}) Hata: {e}")
                    time.sleep(wait_time)  # Backoff delay
                    if attempt == max_retries:
                        self.circuit_breaker[model_key] = True  # Circuit breaker devreye girer

        # TÃ¼m denemeler baÅŸarÄ±sÄ±z olursa None dÃ¶ndÃ¼r
        config.logger.critical(f"ğŸš¨ Embedding iÅŸlemi tamamen baÅŸarÄ±sÄ±z oldu! (PDF: {pdf_id}, Chunk: {chunk_index}/{total_chunks})")
        return {"embedding": None, "model": "failed"}

    def process_large_text(self, text, pdf_id, chunk_size=256, method="words"):
        """
        ğŸ“Œ BÃ¼yÃ¼k metinleri parÃ§alara ayÄ±rarak her bir parÃ§a iÃ§in embedding oluÅŸturur.
        
        Args:
            text (str): Ä°ÅŸlenecek bÃ¼yÃ¼k metin.
            pdf_id (str): PDF dosya kimliÄŸi.
            chunk_size (int): Her parÃ§a iÃ§in maksimum kelime sayÄ±sÄ± (varsayÄ±lan: 256).
            method (str): BÃ¶lme yÃ¶ntemi; "words" kelime bazÄ±nda, "paragraphs" ise paragraf bazÄ±nda bÃ¶lme.
        
        Returns:
            list: OluÅŸturulan embedding vektÃ¶rlerinin listesi.
        """
        chunks = self.split_text(text, chunk_size, method)
        embeddings = []
        total_chunks = len(chunks)

        with ThreadPoolExecutor() as executor:
            futures = {
                executor.submit(
                    self.robust_embed_text,
                    chunk,
                    pdf_id,
                    i,
                    total_chunks
                ): chunk for i, chunk in enumerate(chunks)
            }
            for future in as_completed(futures):
                result = future.result()
                embeddings.append(result)
                time.sleep(RATE_LIMIT_DELAY)  # API rate limit korumasÄ±

        return embeddings

    def save_embeddings(self, embeddings, output_path):
        """
        ğŸ“Œ Embedding verilerini JSON formatÄ±nda kaydeder.
        
        Args:
            embeddings (list): Kaydedilecek embedding verileri.
            output_path (str): Kaydedilecek dosyanÄ±n yolu.
        """
        try:
            with open(output_path, "w", encoding="utf-8") as f:
                json.dump(embeddings, f, ensure_ascii=False, indent=4)
            config.logger.info(f"âœ… Embedding verileri baÅŸarÄ±yla kaydedildi: {output_path}")
        except Exception as e:
            config.logger.error(f"âŒ Embedding verileri kaydedilemedi: {e}")
#---------------------------------------embedding modulu sonu------------------------------------------------
import numpy as np
from sentence_transformers import SentenceTransformer
from config_module import config

# KullanÄ±labilir alternatif embedding modelleri
MODEL_LIST = {
    "contriever_large": "facebook/contriever-large",
    "specter_large": "allenai-specter-large",
    "specter": "allenai/specter",
    "all_mpnet": "sentence-transformers/all-mpnet-base-v2",
    "paraphrase_mpnet": "sentence-transformers/paraphrase-mpnet-base-v2",
    "stsb_roberta_large": "sentence-transformers/stsb-roberta-large",
    "labse": "sentence-transformers/LaBSE",
    "universal_sentence_encoder": "universal-sentence-encoder",  # TF Hub modeli veya benzeri
    "universal_sentence_encoder_lite": "universal-sentence-encoder-lite"
}

def get_sentence_transformer(model_key):
    """
    ğŸ“Œ Belirtilen model anahtarÄ±na gÃ¶re SentenceTransformer modelini yÃ¼kler.
    
    Args:
        model_key (str): MODEL_LIST iÃ§inde yer alan model anahtarÄ±.
    
    Returns:
        SentenceTransformer veya None: YÃ¼klenmiÅŸ model, yÃ¼klenemezse None.
    """
    model_name = MODEL_LIST.get(model_key)
    if not model_name:
        raise ValueError(f"âŒ GeÃ§ersiz model anahtarÄ±: {model_key}")
    try:
        model = SentenceTransformer(model_name)
        config.logger.info(f"âœ… {model_key} modeli yÃ¼klendi (model adÄ±: {model_name}).")
        return model
    except Exception as e:
        config.logger.error(f"âŒ Model yÃ¼klenirken hata oluÅŸtu ({model_key}): {e}")
        return None

def embed_text_with_model(text, model_key):
    """
    ğŸ“Œ Belirtilen alternatif embedding modeli ile metin embedding'i oluÅŸturur.
    
    Args:
        text (str): Embedding oluÅŸturulacak metin.
        model_key (str): KullanÄ±lacak model anahtarÄ± (MODEL_LIST iÃ§inde).
    
    Returns:
        list veya None: Embedding vektÃ¶rÃ¼ (Ã¶rneÄŸin, liste formatÄ±nda) veya hata durumunda None.
    """
    model = get_sentence_transformer(model_key)
    if not model:
        config.logger.error(f"âŒ Model {model_key} yÃ¼klenemedi, embedding oluÅŸturulamÄ±yor.")
        return None
    try:
        embedding = model.encode(text)
        config.logger.info(f"âœ… Embedding oluÅŸturuldu ({model_key}).")
        return embedding.tolist()
    except Exception as e:
        config.logger.error(f"âŒ Embedding oluÅŸturulamadÄ± ({model_key}): {e}")
        return None

def get_available_models():
    """
    ğŸ“Œ KullanÄ±labilir alternatif embedding modellerinin anahtarlarÄ±nÄ± dÃ¶ndÃ¼rÃ¼r.
    
    Returns:
        list: MODEL_LIST iÃ§indeki model anahtarlarÄ±nÄ±n listesi.
    """
    return list(MODEL_LIST.keys())
#---------------------------------------alternative_embedding modulu sonu------------------------------------------------
import time
from config_module import config
from alternative_embedding_module import embed_text_with_model, get_available_models

def robust_embed_text(text, pdf_id, chunk_index, total_chunks, model_priority=None, max_retries=3, backoff_factor=1.0):
    """
    Robust embedding oluÅŸturma fonksiyonu.
    
    Bu fonksiyon, verilen metin iÃ§in Ã¶ncelikle model_priority listesinde yer alan modelleri
    sÄ±rasÄ±yla dener. Her model iÃ§in maksimum max_retries deneme yapÄ±lÄ±r; baÅŸarÄ±sÄ±z olursa
    exponential backoff uygulanÄ±r ve sonraki modele geÃ§ilir.
    
    Args:
        text (str): Embedding oluÅŸturulacak metin.
        pdf_id (str): PDF dosyasÄ±nÄ±n temel ID'si.
        chunk_index (int): Ä°ÅŸlenen chunk numarasÄ± (0 tabanlÄ±).
        total_chunks (int): Toplam chunk sayÄ±sÄ±.
        model_priority (list, optional): Denenecek modellerin listesi. VarsayÄ±lan olarak, tÃ¼m alternatif modeller.
        max_retries (int, optional): Her model iÃ§in maksimum deneme sayÄ±sÄ± (varsayÄ±lan: 3).
        backoff_factor (float, optional): Denemeler arasÄ±nda kullanÄ±lacak exponential backoff katsayÄ±sÄ± (varsayÄ±lan: 1.0).
    
    Returns:
        list veya None: BaÅŸarÄ±lÄ± ise embedding vektÃ¶rÃ¼ (liste formatÄ±nda), aksi halde None.
    """
    if model_priority is None:
        model_priority = get_available_models()
    
    for model in model_priority:
        config.logger.info(f"Denenecek model: {model} (PDF: {pdf_id}, Chunk: {chunk_index+1}/{total_chunks})")
        attempt = 0
        while attempt < max_retries:
            try:
                embedding = embed_text_with_model(text, model)
                if embedding is not None:
                    config.logger.info(f"âœ… Model {model} baÅŸarÄ±lÄ±: PDF {pdf_id}, Chunk {chunk_index+1}/{total_chunks}")
                    # Burada, istenirse yerel cache (Ã¶rn. embedding_cache.json) gÃ¼ncellenebilir.
                    return embedding
            except Exception as e:
                config.logger.error(f"âŒ Hata: Model {model}, Deneme {attempt+1}/{max_retries}: {e}")
            attempt += 1
            sleep_time = backoff_factor * (2 ** attempt)
            config.logger.info(f"â³ Bekleniyor: {sleep_time} saniye")
            time.sleep(sleep_time)
        config.logger.warning(f"âš ï¸ Model {model} iÃ§in maksimum deneme sayÄ±sÄ±na ulaÅŸÄ±ldÄ±, sonraki modele geÃ§iliyor.")
    
    config.logger.error(f"âŒ TÃ¼m modeller baÅŸarÄ±sÄ±z oldu: PDF {pdf_id}, Chunk {chunk_index+1}/{total_chunks}")
    return None
#---------------------------------------robust_embedding modulu sonu------------------------------------------------

import os
import re
import json
import psutil
import threading
from rapidfuzz import fuzz
from config_module import config

def memory_usage():
    """
    Bellek kullanÄ±mÄ±nÄ± MB cinsinden dÃ¶ndÃ¼rÃ¼r.

    Returns:
        float: Mevcut sÃ¼recin kullanÄ±lan bellek miktarÄ± (MB cinsinden).
    """
    process = psutil.Process(os.getpid())
    mem_info = process.memory_info()
    return mem_info.rss / (1024 ** 2)

def shorten_title(title, max_length=80):
    """
    Uzun baÅŸlÄ±klarÄ± belirtilen maksimum uzunlukta kÄ±saltÄ±r.

    Args:
        title (str): KÄ±saltÄ±lacak baÅŸlÄ±k.
        max_length (int): Maksimum karakter sayÄ±sÄ± (varsayÄ±lan: 80).

    Returns:
        str: KÄ±saltÄ±lmÄ±ÅŸ baÅŸlÄ±k; eÄŸer baÅŸlÄ±k max_length'ten kÄ±sa ise deÄŸiÅŸiklik yapmaz.
    """
    if len(title) <= max_length:
        return title
    else:
        return title[:max_length] + "..."

def clean_advanced_text(text):
    """
    GeliÅŸmiÅŸ metin temizleme fonksiyonu:
    - HTML etiketlerini kaldÄ±rÄ±r.
    - Markdown link yapÄ±larÄ±nÄ± temizler.
    - Sayfa baÅŸÄ±/sonu ifadelerini (Ã¶rn. "Page 1", "Sayfa 1") kaldÄ±rÄ±r.
    - Fazla boÅŸluklarÄ± tek boÅŸluk haline getirir.
    - KÄ±rpÄ±lmÄ±ÅŸ kelimelerdeki tire iÅŸaretlerini dÃ¼zeltir.

    Args:
        text (str): Temizlenecek ham metin.

    Returns:
        str: TemizlenmiÅŸ metin.
    """
    # HTML etiketlerini kaldÄ±r
    text = re.sub(r"<[^>]+>", " ", text)
    # Markdown link yapÄ±larÄ±nÄ± kaldÄ±r (Ã¶rneÄŸin, [Link](url))
    text = re.sub(r"\[[^\]]+\]\([^)]+\)", " ", text)
    # Sayfa baÅŸÄ±/sonu ifadelerini kaldÄ±r (Ã¶rn. "Page 1" veya "Sayfa 1")
    text = re.sub(r"(Page|Sayfa)\s*\d+", " ", text, flags=re.IGNORECASE)
    # Fazla boÅŸluklarÄ± tek boÅŸluk yap
    text = re.sub(r"\s{2,}", " ", text)
    # KÄ±rpÄ±lmÄ±ÅŸ kelimelerdeki tireyi kaldÄ±r (Ã¶rneÄŸin, "infor- mation" -> "information")
    text = re.sub(r"(\w+)-\s+(\w+)", r"\1\2", text)
    return text.strip()

def fuzzy_match(text1, text2):
    """
    RapidFuzz kÃ¼tÃ¼phanesini kullanarak iki metin arasÄ±ndaki benzerlik oranÄ±nÄ± hesaplar.

    Args:
        text1 (str): Ä°lk metin.
        text2 (str): Ä°kinci metin.

    Returns:
        float: Benzerlik oranÄ± (0-100 arasÄ± deÄŸer).
    """
    return fuzz.ratio(text1, text2)

# --- Stack YÃ¶netimi (Ä°ÅŸlem Listesi) ---

# Stack dosyasÄ±nÄ±n yolu config Ã¼zerinden belirleniyor.
STACK_DOSYASI = config.STACK_DOSYASI
stack_lock = threading.Lock()

def stack_yukle():
    """
    Stack dosyasÄ±nÄ± (iÅŸlem listesi) yÃ¼kler.

    Returns:
        list: Ä°ÅŸlenen dosyalarÄ±n listesini iÃ§eren liste.
    """
    if os.path.exists(STACK_DOSYASI):
        try:
            with open(STACK_DOSYASI, "r", encoding="utf-8") as f:
                return json.load(f)
        except json.JSONDecodeError:
            config.logger.error("âŒ Stack dosyasÄ± bozuk, sÄ±fÄ±rlanÄ±yor.")
    return []

def stack_guncelle(dosya_adi, islem):
    """
    Stack dosyasÄ±nÄ± gÃ¼nceller. Ä°ÅŸlem "ekle" ise dosya adÄ±nÄ± ekler; "sil" ise Ã§Ä±karÄ±r.

    Args:
        dosya_adi (str): GÃ¼ncellenecek dosya adÄ±.
        islem (str): "ekle" veya "sil".
    """
    with stack_lock:
        stack = stack_yukle()
        if islem == "ekle":
            if dosya_adi not in stack:
                stack.append(dosya_adi)
        elif islem == "sil":
            if dosya_adi in stack:
                stack.remove(dosya_adi)
        with open(STACK_DOSYASI, "w", encoding="utf-8") as f:
            json.dump(stack, f, ensure_ascii=False, indent=2)

#---------------------------------------helper modulu sonu------------------------------------------------

import os
import re
import threading
from datetime import datetime
from pathlib import Path
import multiprocessing
from tqdm import tqdm
import chromadb
from openai import OpenAI

from config_module import config
from zotero_module import dokuman_id_al, fetch_zotero_metadata
from pdf_processing import (
    extract_text_from_pdf,
    reflow_columns,
    map_scientific_sections_extended,
    map_pdf_before_extraction,
    detect_columns,
    extract_references_enhanced
)
from embedding_module import embed_text
from helper_module import stack_yukle, stack_guncelle, shorten_title

class IslemYoneticisi:
    """
    PDF/TXT dosyalarÄ±nÄ± iÅŸleme sÃ¼recini yÃ¶neten ana sÄ±nÄ±f.
    
    Ä°ÅŸ AkÄ±ÅŸÄ±:
      1. Dosya tipi (.pdf veya .txt) belirlenir.
      2. Ä°ÅŸleme baÅŸlamadan Ã¶nce, dosya adÄ± stack'e eklenir.
      3. PDF ise: 
         - Ä°lk olarak map_pdf_before_extraction ile yapÄ±sal haritalama yapÄ±lÄ±r.
         - extract_text_from_pdf ile ham metin Ã§Ä±karÄ±lÄ±r.
         TXT ise: dosya doÄŸrudan okunur ve map_scientific_sections_extended ile bÃ¶lÃ¼mler haritalanÄ±r.
      4. Ã‡Ä±karÄ±lan metin, reflow_columns ile tek akÄ±ÅŸa dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼r.
      5. Bilimsel bÃ¶lÃ¼mler map_scientific_sections_extended ile yeniden tespit edilir.
      6. SÃ¼tun yapÄ±sÄ± detect_columns ile belirlenir.
      7. KaynakÃ§a extract_references_enhanced ile Ã§Ä±karÄ±lÄ±r.
      8. Zotero entegrasyonu: dokuman_id_al ile temel dosya ID'si alÄ±nÄ±r, shorten_title ile kÄ±saltÄ±lÄ±r, fetch_zotero_metadata ile bibliyometrik veriler Ã§ekilir.
      9. Temiz metin Ã¼zerinden embed_text fonksiyonu ile embedding oluÅŸturulur.
     10. TÃ¼m bu bilgiler, bir sonuÃ§ sÃ¶zlÃ¼ÄŸÃ¼nde toplanÄ±r.
     11. Ä°ÅŸlem sonunda, dosya adÄ± stack'ten kaldÄ±rÄ±lÄ±r ve sayaÃ§lar gÃ¼ncellenir.
    
    DÃ¶ndÃ¼rdÃ¼ÄŸÃ¼ sonuÃ§ sÃ¶zlÃ¼ÄŸÃ¼, iÅŸlenmiÅŸ dosyaya ait tÃ¼m bilgileri iÃ§erir.
    """
    def __init__(self):
        self.stack_lock = threading.Lock()
        self.kume_sonuclari = []  # Ä°ÅŸlemden elde edilen sonuÃ§larÄ±n saklandÄ±ÄŸÄ± liste
        self.sayaÃ§lar = {
            'toplam': 0,
            'baÅŸarÄ±lÄ±': 0,
            'hata': 0
        }
        # ChromaDB baÄŸlantÄ±sÄ± ve koleksiyonlarÄ±nÄ±n oluÅŸturulmasÄ±
        self.chroma_client = chromadb.PersistentClient(path="chroma_db")
        self.koleksiyon = self.chroma_client.get_or_create_collection(name="pdf_embeddings")
        self.zotero_koleksiyon = self.chroma_client.get_or_create_collection(name="zotero_meta")
        self.secili_dosya = None

    def pdf_txt_isle(self, dosya_yolu):
        try:
            # Ä°ÅŸleme baÅŸlamadan Ã¶nce stack gÃ¼ncellemesi
            self.stack_guncelle(dosya_yolu.name, "ekle")
            config.logger.info(f"{dosya_yolu.name} iÅŸleme baÅŸladÄ±.")
            
            ext = dosya_yolu.suffix.lower()
            if ext == ".pdf":
                # PDF iÃ§in: Ã¶nce yapÄ±sal haritalama, sonra metin Ã§Ä±karÄ±mÄ±
                harita = map_pdf_before_extraction(dosya_yolu, method=config.PDF_TEXT_EXTRACTION_METHOD)
                ham_metin = extract_text_from_pdf(dosya_yolu, method=config.PDF_TEXT_EXTRACTION_METHOD)
            elif ext == ".txt":
                with open(dosya_yolu, "r", encoding="utf-8") as f:
                    ham_metin = f.read()
                harita = map_scientific_sections_extended(ham_metin)
            else:
                config.logger.error(f"Desteklenmeyen dosya uzantÄ±sÄ±: {dosya_yolu}")
                return None

            if not ham_metin:
                raise ValueError("Ham metin Ã§Ä±karÄ±lamadÄ±.")

            # Metni tek akÄ±ÅŸa dÃ¶nÃ¼ÅŸtÃ¼rme (reflow)
            temiz_metin = reflow_columns(ham_metin)

            # Bilimsel bÃ¶lÃ¼mlerin haritalanmasÄ±
            bolum_haritasi = map_scientific_sections_extended(ham_metin)

            # SÃ¼tun yapÄ±sÄ± tespiti
            sutun_bilgisi = detect_columns(ham_metin)

            # KaynakÃ§a Ã§Ä±karÄ±mÄ± (hata yÃ¶netimiyle)
            try:
                references = extract_references_enhanced(ham_metin)
            except Exception as e:
                config.logger.error(f"KaynakÃ§a Ã§Ä±karÄ±m hatasÄ±: {e}")
                references = []

            # Zotero entegrasyonu: Dosya temel ID'si alÄ±nÄ±r ve kÄ±saltÄ±lÄ±r
            dosya_id = dokuman_id_al(dosya_yolu.name)
            if not dosya_id:
                dosya_id = dosya_yolu.stem
            dosya_id = shorten_title(dosya_id, max_length=80)
            zotero_meta = fetch_zotero_metadata(dosya_id)

            # Embedding oluÅŸturma (temiz metin Ã¼zerinden)
            embedding = embed_text(temiz_metin)

            # SonuÃ§ sÃ¶zlÃ¼ÄŸÃ¼ hazÄ±rlanÄ±yor
            result = {
                "dosya": dosya_yolu.name,
                "ham_metin": ham_metin,
                "temiz_metin": temiz_metin,
                "harita": harita,
                "bolum_haritasi": bolum_haritasi,
                "sutun_bilgisi": sutun_bilgisi,
                "kaynakca": references,
                "zotero_meta": zotero_meta,
                "embedding": embedding,
                "islem_tarihi": datetime.now().isoformat()
            }

            self.stack_guncelle(dosya_yolu.name, "sil")
            self.sayaÃ§lar['baÅŸarÄ±lÄ±'] += 1
            config.logger.info(f"{dosya_yolu.name} baÅŸarÄ±yla iÅŸlendi.")
            return result
        except Exception as e:
            self.sayaÃ§lar['hata'] += 1
            config.logger.error(f"{dosya_yolu.name} iÅŸlenirken hata: {e}", exc_info=True)
            return None

    def stack_guncelle(self, dosya_adi, islem):
        """
        Stack gÃ¼ncelleme iÅŸlemini helper_module Ã¼zerinden gerÃ§ekleÅŸtirir.
        """
        from helper_module import stack_guncelle
        with self.stack_lock:
            stack_guncelle(dosya_adi, islem)
 
 #---------------------------------------processing_manager modulu sonu------------------------------------------------

import os
import json
import csv
import pandas as pd
from pathlib import Path
from config_module import config

def save_text_file(directory, filename, content):
    """
    Genel metin dosyalarÄ±nÄ± TXT formatÄ±nda kaydeder.
    
    Args:
        directory (str or Path): DosyanÄ±n kaydedileceÄŸi dizin.
        filename (str): Dosya adÄ± (uzantÄ± eklenmeyecek, fonksiyon ekleyecek).
        content (str): Kaydedilecek metin iÃ§eriÄŸi.
    
    Returns:
        str: OluÅŸturulan dosya yolunu dÃ¶ndÃ¼rÃ¼r.
    """
    directory = Path(directory)
    directory.mkdir(parents=True, exist_ok=True)
    file_path = directory / f"{filename}.txt"
    try:
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(content)
        config.logger.info(f"TXT dosyasÄ± kaydedildi: {file_path}")
        return str(file_path)
    except Exception as e:
        config.logger.error(f"TXT dosyasÄ± kaydedilemedi: {file_path}, Hata: {e}")
        return None

def save_json_file(directory, filename, content):
    """
    Veriyi JSON formatÄ±nda kaydeder.
    
    Args:
        directory (str or Path): DosyanÄ±n kaydedileceÄŸi dizin.
        filename (str): Dosya adÄ± (uzantÄ± eklenmeyecek).
        content (dict): Kaydedilecek veri.
    
    Returns:
        str: OluÅŸturulan dosya yolunu dÃ¶ndÃ¼rÃ¼r.
    """
    directory = Path(directory)
    directory.mkdir(parents=True, exist_ok=True)
    file_path = directory / f"{filename}.json"
    try:
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(content, f, ensure_ascii=False, indent=4)
        config.logger.info(f"JSON dosyasÄ± kaydedildi: {file_path}")
        return str(file_path)
    except Exception as e:
        config.logger.error(f"JSON dosyasÄ± kaydedilemedi: {file_path}, Hata: {e}")
        return None

def save_clean_text_files(original_filename, clean_text, bib_info):
    """
    Temiz metin dosyasÄ±nÄ± hem TXT hem de JSON formatÄ±nda, bibliyometrik bilgilerle birlikte kaydeder.
    
    Dosya isimlendirme: {ID}.clean.txt ve {ID}.clean.meta.json
    ID, dosyanÄ±n temel adÄ± (dokuman_id_al + shorten_title ile elde edilir).
    
    Args:
        original_filename (str): Ä°ÅŸlenen dosyanÄ±n orijinal adÄ±.
        clean_text (str): TemizlenmiÅŸ metin.
        bib_info (dict): Bibliyometrik bilgiler.
    
    Returns:
        dict: {"txt": dosya_yolu, "json": dosya_yolu} ÅŸeklinde dosya yollarÄ±.
    """
    base_name = Path(original_filename).stem
    txt_path = save_text_file(config.CLEAN_TEXT_DIR / "txt", f"{base_name}.clean", clean_text)
    json_path = save_json_file(config.CLEAN_TEXT_DIR / "json", f"{base_name}.clean.meta", bib_info)
    return {"txt": txt_path, "json": json_path}

def save_references_files(original_filename, references, bib_info):
    """
    KaynakÃ§a verilerini farklÄ± formatlarda kaydeder:
      - TXT: {ID}.references.txt
      - JSON: {ID}.references.meta.json
      - VOSviewer: {ID}.vos.references.txt
      - Pajek: {ID}.pjk.references.paj
      - CSV: {ID}.references.csv  (Her satÄ±rda bir kaynakÃ§a kaydÄ±)
    
    Args:
        original_filename (str): Orijinal dosya adÄ±.
        references (list): KaynakÃ§a metinlerinin listesi.
        bib_info (dict): Bibliyometrik bilgiler.
    
    Returns:
        dict: Kaydedilen dosya yollarÄ±nÄ± iÃ§eren sÃ¶zlÃ¼k.
    """
    base_name = Path(original_filename).stem
    ref_txt = save_text_file(config.REFERENCES_DIR / "txt", f"{base_name}.references", "\n".join(references))
    ref_json = save_json_file(config.REFERENCES_DIR / "json", f"{base_name}.references.meta", bib_info)
    # VOSviewer formatÄ±: Basit liste, her satÄ±rda bir kaynak
    vos_path = save_text_file(config.REFERENCES_DIR / "vosviewer", f"{base_name}.vos.references", "\n".join(references))
    # Pajek formatÄ±: Ä°lk satÄ±rda toplam vertex sayÄ±sÄ±, sonraki satÄ±rlarda id ve kaynakÃ§a metni
    pajek_file = config.REFERENCES_DIR / "pajek" / f"{base_name}.pjk.references.paj"
    (config.REFERENCES_DIR / "pajek").mkdir(parents=True, exist_ok=True)
    try:
        with open(pajek_file, 'w', encoding='utf-8') as f:
            f.write(f"*Vertices {len(references)}\n")
            for idx, ref in enumerate(references, start=1):
                f.write(f'{idx} "{ref}"\n')
        config.logger.info(f"Pajek dosyasÄ± kaydedildi: {pajek_file}")
        pajek_path = str(pajek_file)
    except Exception as e:
        config.logger.error(f"Pajek dosyasÄ± kaydedilemedi: {pajek_file}, Hata: {e}")
        pajek_path = None

    # CSV formatÄ±: Her kaynakÃ§a iÃ§in bir satÄ±r
    csv_file = config.REFERENCES_DIR / "csv" / f"{base_name}.references.csv"
    (config.REFERENCES_DIR / "csv").mkdir(parents=True, exist_ok=True)
    try:
        with open(csv_file, 'w', encoding='utf-8', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(["KaynakÃ§a"])
            for ref in references:
                writer.writerow([ref])
        config.logger.info(f"CSV dosyasÄ± kaydedildi: {csv_file}")
        csv_path = str(csv_file)
    except Exception as e:
        config.logger.error(f"CSV dosyasÄ± kaydedilemedi: {csv_file}, Hata: {e}")
        csv_path = None

    return {"txt": ref_txt, "json": ref_json, "vosviewer": vos_path, "pajek": pajek_path, "csv": csv_path}

def save_table_files(original_filename, table_data_list):
    """
    TablolarÄ± JSON, CSV ve Excel formatlarÄ±nda kaydeder.
    
    Args:
        original_filename (str): Orijinal dosya adÄ±.
        table_data_list (list): TablolarÄ±n verilerini iÃ§eren liste. Her tablo, 'baslik' ve 'veriler' anahtarlarÄ±na sahip sÃ¶zlÃ¼k olarak tanÄ±mlanmalÄ±.
    
    Returns:
        dict: Kaydedilen dosya yollarÄ±nÄ± iÃ§eren sÃ¶zlÃ¼k.
    """
    base_name = Path(original_filename).stem
    table_dir = config.TABLES_DIR
    table_dir.mkdir(parents=True, exist_ok=True)

    # JSON formatÄ±nda kaydet
    json_path = table_dir / f"{base_name}.tables.json"
    try:
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(table_data_list, f, ensure_ascii=False, indent=4)
        config.logger.info(f"Tablolar JSON formatÄ±nda kaydedildi: {json_path}")
    except Exception as e:
        config.logger.error(f"JSON dosyasÄ± kaydedilemedi: {json_path}, Hata: {e}")
    
    # CSV formatÄ±nda kaydet
    csv_path = table_dir / f"{base_name}.tables.csv"
    try:
        with open(csv_path, 'w', encoding='utf-8', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(["Tablo BaÅŸlÄ±ÄŸÄ±", "Tablo Ä°Ã§eriÄŸi"])
            for table in table_data_list:
                writer.writerow([table.get("baslik", ""), json.dumps(table.get("veriler", []), ensure_ascii=False)])
        config.logger.info(f"Tablolar CSV formatÄ±nda kaydedildi: {csv_path}")
    except Exception as e:
        config.logger.error(f"CSV dosyasÄ± kaydedilemedi: {csv_path}, Hata: {e}")

    # Excel formatÄ±nda kaydet
    excel_path = table_dir / f"{base_name}.tables.xlsx"
    try:
        writer = pd.ExcelWriter(excel_path, engine='xlsxwriter')
        for idx, table in enumerate(table_data_list, start=1):
            df = pd.DataFrame(table.get("veriler", []))
            sheet_name = f"Tablo{idx}"
            df.to_excel(writer, sheet_name=sheet_name, index=False)
        writer.save()
        config.logger.info(f"Tablolar Excel formatÄ±nda kaydedildi: {excel_path}")
    except Exception as e:
        config.logger.error(f"Excel dosyasÄ± kaydedilemedi: {excel_path}, Hata: {e}")

    return {"json": str(json_path), "csv": str(csv_path), "excel": str(excel_path)}

def save_embedding_file(original_filename, embedding_text, chunk_index):
    """
    Her dosyanÄ±n embedding verilerini kaydeder.
    Dosya isimlendirme: {ID}_chunk{chunk_index}.embed.txt
    
    Args:
        original_filename (str): Orijinal dosya adÄ±.
        embedding_text (str): Embedding verisinin kaydedilecek metin hali.
        chunk_index (int): Chunk numarasÄ±.
    
    Returns:
        str: OluÅŸturulan dosya yolunu dÃ¶ndÃ¼rÃ¼r.
    """
    base_name = Path(original_filename).stem
    embedding_dir = config.EMBEDDINGS_DIR
    embedding_dir.mkdir(parents=True, exist_ok=True)
    file_path = embedding_dir / f"{base_name}_chunk{chunk_index}.embed.txt"
    try:
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(embedding_text)
        config.logger.info(f"Embedding dosyasÄ± kaydedildi: {file_path}")
        return str(file_path)
    except Exception as e:
        config.logger.error(f"Embedding dosyasÄ± kaydedilemedi: {file_path}, Hata: {e}")
        return None

def save_chunked_text_files(original_filename, full_text, chunk_size=256):
    """
    BÃ¼yÃ¼k metni, belirlenen chunk boyutuna gÃ¶re bÃ¶lerek dosya sistemine kaydeder.
    Dosya isimlendirme: {ID}_part{parÃ§a_numarasÄ±}.txt
    
    Args:
        original_filename (str): Orijinal dosya adÄ±.
        full_text (str): TÃ¼m metin.
        chunk_size (int): Her chunk iÃ§in karakter sayÄ±sÄ± (varsayÄ±lan: 256).
    
    Returns:
        list: Kaydedilen tÃ¼m dosya yollarÄ±nÄ±n listesi.
    """
    base_name = Path(original_filename).stem
    chunk_dir = config.CLEAN_TEXT_DIR / "chunks"
    chunk_dir.mkdir(parents=True, exist_ok=True)
    text_chunks = [full_text[i:i + chunk_size] for i in range(0, len(full_text), chunk_size)]
    file_paths = []
    for idx, chunk in enumerate(text_chunks, start=1):
        file_path = save_text_file(chunk_dir, f"{base_name}_part{idx}", chunk)
        if file_path:
            file_paths.append(file_path)
    config.logger.info(f"BÃ¼yÃ¼k metin {len(text_chunks)} parÃ§aya bÃ¶lÃ¼ndÃ¼ ve kaydedildi.")
    return file_paths
#---------------------------------------file_save_module sonu------------------------------------------------
import re
import json
from pathlib import Path
from rapidfuzz import fuzz
from config_module import config
from helper_module import fuzzy_match  # RapidFuzz tabanlÄ± benzerlik hesaplamasÄ±

def split_into_sentences(text):
    """
    ğŸ“Œ Metni cÃ¼mlelere bÃ¶ler ve her cÃ¼mleye sÄ±ra numarasÄ± ekler.
    
    Args:
        text (str): Ä°ÅŸlenecek metin.
    
    Returns:
        list: Her biri {'id': cÃ¼mle numarasÄ±, 'text': cÃ¼mle} iÃ§eren sÃ¶zlÃ¼klerin listesi.
    """
    # Noktalama iÅŸaretleri sonrasÄ±nda bÃ¶l
    sentences = re.split(r'(?<=[.!?])\s+', text)
    return [{"id": idx, "text": sentence.strip()} for idx, sentence in enumerate(sentences, start=1) if sentence.strip()]

def extract_citations_from_sentence(sentence):
    """
    ğŸ“Œ CÃ¼mledeki atÄ±f ifadelerini tespit eder.
    Regex desenleri ile farklÄ± atÄ±f stillerini yakalar.
    
    Args:
        sentence (str): Ä°ÅŸlenecek cÃ¼mle.
    
    Returns:
        list: Tespit edilen atÄ±f ifadelerinin listesi.
    """
    # Ã–rnek regex desenleri; daha geliÅŸmiÅŸ stiller eklenebilir.
    citation_patterns = [
        r"\(([\w\s\.,\-]+, \d{4})\)",   # Ã–rneÄŸin: (Smith, 2020)
        r"\[\d+\]",                    # Ã–rneÄŸin: [12]
        r"\b(?:[A-Z][a-z]+ et al\.?, \d{4})\b"  # Ã–rneÄŸin: Smith et al., 2020
    ]
    citations = []
    for pattern in citation_patterns:
        matches = re.findall(pattern, sentence)
        if matches:
            citations.extend(matches)
    # TekrarlarÄ± kaldÄ±r
    return list(set(citations))

def match_citation_with_references(citation_marker, references):
    """
    ğŸ“Œ Tespit edilen atÄ±f ifadesini, kaynakÃ§a listesiyle eÅŸleÅŸtirir.
    Ã–nce tam eÅŸleÅŸme, ardÄ±ndan fuzzy matching (RapidFuzz) kullanÄ±lÄ±r.
    
    Args:
        citation_marker (str): AtÄ±f ifadesi.
        references (list): KaynakÃ§a metinlerinin listesi.
    
    Returns:
        str veya None: EÅŸleÅŸen referans bulunursa dÃ¶ner, bulunamazsa None.
    """
    # Tam eÅŸleÅŸme
    for ref in references:
        if citation_marker.lower() in ref.lower():
            return ref
    
    # Fuzzy matching
    best_match = None
    best_score = 0
    for ref in references:
        score = fuzz.ratio(citation_marker.lower(), ref.lower())
        if score > best_score:
            best_match = ref
            best_score = score
    return best_match if best_score >= 85 else None

def get_section_for_sentence(sentence_id, section_info):
    """
    ğŸ“Œ CÃ¼mlenin ait olduÄŸu bilimsel bÃ¶lÃ¼mÃ¼ belirler.
    
    Args:
        sentence_id (int): CÃ¼mlenin sÄ±ra numarasÄ±.
        section_info (dict): BÃ¶lÃ¼m haritalama bilgileri (Ã¶rneÄŸin, {"Introduction": {"start": 5, "end": 20, ...}, ...}).
    
    Returns:
        str: CÃ¼mlenin ait olduÄŸu bÃ¶lÃ¼m veya "Unknown".
    """
    for section, info in section_info.items():
        if info and info.get("start") is not None and info.get("end") is not None:
            if info["start"] <= sentence_id <= info["end"]:
                return section
    return "Unknown"

def map_citations(clean_text, bibliography, section_info):
    """
    ğŸ“Œ Temiz metin iÃ§erisindeki tÃ¼m cÃ¼mleleri iÅŸleyerek atÄ±f mapping yapÄ±sÄ±nÄ± oluÅŸturur.
    Her cÃ¼mle iÃ§in; cÃ¼mle numarasÄ±, metni, tespit edilen atÄ±f(lar), eÅŸleÅŸen referans ve bÃ¶lÃ¼m bilgisi dÃ¶ndÃ¼rÃ¼lÃ¼r.
    
    Args:
        clean_text (str): TemizlenmiÅŸ makale metni.
        bibliography (list): KaynakÃ§a referanslarÄ±nÄ±n listesi.
        section_info (dict): BÃ¶lÃ¼m haritalama bilgileri.
    
    Returns:
        list: AtÄ±f mapping verilerini iÃ§eren sÃ¶zlÃ¼klerin listesi.
    """
    mapped_citations = []
    sentences = split_into_sentences(clean_text)
    for sentence_obj in sentences:
        sent_id = sentence_obj["id"]
        text = sentence_obj["text"]
        citations = extract_citations_from_sentence(text)
        for citation in citations:
            matched_ref = match_citation_with_references(citation, bibliography)
            mapped_citations.append({
                "sentence_id": sent_id,
                "sentence": text,
                "citation": citation,
                "matched_reference": matched_ref,
                "section": get_section_for_sentence(sent_id, section_info)
            })
    return mapped_citations

def save_citation_mapping(pdf_id, citation_mapping):
    """
    ğŸ“Œ OluÅŸturulan Citation Mapping verilerini, bibliyometrik bilgilerle birlikte "ID.citation.json" olarak kaydeder.
    
    Args:
        pdf_id (str): PDF dosya ID'si.
        citation_mapping (list): Citation mapping verilerini iÃ§eren liste.
    
    Returns:
        str: OluÅŸturulan dosya yolunu dÃ¶ndÃ¼rÃ¼r.
    """
    citation_dir = Path(config.SUCCESS_DIR) / "citations"
    citation_dir.mkdir(parents=True, exist_ok=True)
    file_path = citation_dir / f"{pdf_id}.citation.json"
    try:
        with open(file_path, "w", encoding="utf-8") as f:
            json.dump(citation_mapping, f, ensure_ascii=False, indent=4)
        config.logger.info(f"âœ… Citation Mapping kaydedildi: {file_path}")
        return str(file_path)
    except Exception as e:
        config.logger.error(f"âŒ Citation Mapping kaydedilemedi: {file_path}, Hata: {e}")
        return None

def load_citation_mapping(pdf_id):
    """
    ğŸ“Œ Daha Ã¶nce kaydedilmiÅŸ Citation Mapping dosyasÄ±nÄ± yÃ¼kler.
    
    Args:
        pdf_id (str): PDF dosya ID'si.
    
    Returns:
        list veya None: Citation mapping verisi, bulunamazsa veya hata olursa None.
    """
    citation_dir = Path(config.SUCCESS_DIR) / "citations"
    file_path = citation_dir / f"{pdf_id}.citation.json"
    if not file_path.exists():
        config.logger.error(f"âŒ Citation Mapping dosyasÄ± bulunamadÄ±: {file_path}")
        return None
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception as e:
        config.logger.error(f"âŒ Citation Mapping dosyasÄ± yÃ¼klenemedi: {file_path}, Hata: {e}")
        return None
    #---------------------------------------citation_mapping modulu sonu------------------------------------------------
import os
import json
import customtkinter as ctk
from tkinter import filedialog, messagebox
from pathlib import Path
from processing_manager import IslemYoneticisi
from citation_mapping_module import load_citation_mapping
from embedding_module import embed_text  # Temel embedding oluÅŸturma (arama iÃ§in kullanÄ±labilir)
from clustering_module import perform_clustering  # KÃ¼meleme analizi fonksiyonu
from fine_tuning_module import train_custom_model  # Fine-tuning model eÄŸitimi
from data_query_module import query_data  # GeliÅŸmiÅŸ veri sorgulama fonksiyonu
from config_module import config

class AnaArayuz(ctk.CTk):
    def __init__(self, islem_yoneticisi):
        super().__init__()
        self.islem_yoneticisi = islem_yoneticisi
        self.title("ğŸ“‘ Zotero Entegre PDF Ä°ÅŸleyici")
        self.geometry("1200x800")
        self._arayuzu_hazirla()

    def _arayuzu_hazirla(self):
        """
        Ana GUI bileÅŸenlerini oluÅŸturur.
        """
        # Ãœst bÃ¶lÃ¼mde dosya seÃ§me ve iÅŸlem baÅŸlatma butonlarÄ±
        self.dosya_sec_btn = ctk.CTkButton(self, text="ğŸ“‚ PDF SeÃ§", command=self._dosya_sec)
        self.dosya_sec_btn.pack(pady=10)

        self.baslat_btn = ctk.CTkButton(self, text="ğŸš€ Ä°ÅŸlemi BaÅŸlat", command=self._islem_baslat)
        self.baslat_btn.pack(pady=10)

        self.citation_btn = ctk.CTkButton(self, text="ğŸ“– AtÄ±f Zinciri GÃ¶rÃ¼ntÃ¼le", command=self._atif_goster)
        self.citation_btn.pack(pady=10)

        # Ä°lave Ã¶zellikler menÃ¼sÃ¼
        self.ilave_ozellikler_menusu()

        # SonuÃ§ EkranÄ±
        self.sonuc_ekrani = ctk.CTkTextbox(self, width=1000, height=500)
        self.sonuc_ekrani.pack(pady=10)

        # Durum Ã‡ubuÄŸu (isteÄŸe baÄŸlÄ±)
        self.status_bar = ctk.CTkLabel(self, text="Durum: HazÄ±r", anchor="w")
        self.status_bar.pack(fill="x", padx=10, pady=5)

    def _dosya_sec(self):
        """
        KullanÄ±cÄ±nÄ±n PDF dosyasÄ± seÃ§mesini saÄŸlar.
        """
        dosya_yolu = filedialog.askopenfilename(filetypes=[("PDF Files", "*.pdf")])
        if dosya_yolu:
            self.sonuc_ekrani.insert("end", f"\nğŸ“„ SeÃ§ilen Dosya: {dosya_yolu}\n")
            self.islem_yoneticisi.secili_dosya = dosya_yolu
            self.status_bar.configure(text=f"SeÃ§ilen dosya: {Path(dosya_yolu).name}")

    def _islem_baslat(self):
        """
        SeÃ§ili PDF dosyasÄ± iÅŸlenir.
        """
        if not hasattr(self.islem_yoneticisi, "secili_dosya") or not self.islem_yoneticisi.secili_dosya:
            messagebox.showerror("Hata", "LÃ¼tfen Ã¶nce bir PDF dosyasÄ± seÃ§in!")
            return

        self.sonuc_ekrani.insert("end", "\nâ³ Ä°ÅŸlem baÅŸlatÄ±lÄ±yor...\n")
        self.status_bar.configure(text="Ä°ÅŸlem baÅŸlatÄ±lÄ±yor...")
        basari, sonuc = self.islem_yoneticisi.pdf_txt_isle(Path(self.islem_yoneticisi.secili_dosya))

        if basari:
            self.sonuc_ekrani.insert("end", f"âœ… Ä°ÅŸlem tamamlandÄ±: {self.islem_yoneticisi.secili_dosya}\n")
            self.status_bar.configure(text="Ä°ÅŸlem tamamlandÄ±.")
        else:
            self.sonuc_ekrani.insert("end", f"âŒ Hata oluÅŸtu: {sonuc}\n")
            self.status_bar.configure(text="Hata oluÅŸtu.")

    def _atif_goster(self):
        """
        SeÃ§ili PDF dosyasÄ±nÄ±n atÄ±f zincirini gÃ¶rÃ¼ntÃ¼ler.
        """
        if not hasattr(self.islem_yoneticisi, "secili_dosya") or not self.islem_yoneticisi.secili_dosya:
            messagebox.showerror("Hata", "LÃ¼tfen Ã¶nce bir PDF dosyasÄ± seÃ§in!")
            return

        pdf_id = Path(self.islem_yoneticisi.secili_dosya).stem
        citation_data = load_citation_mapping(pdf_id)

        if citation_data:
            display_text = "\nğŸ“š AtÄ±f Zinciri:\n"
            for entry in citation_data:
                display_text += f"ğŸ”¹ CÃ¼mle {entry['sentence_id']}: {entry['sentence']}\n    EÅŸleÅŸen Referans: {entry['matched_reference']}\n"
            self.sonuc_ekrani.insert("end", display_text)
            self.status_bar.configure(text="AtÄ±f zinciri gÃ¶rÃ¼ntÃ¼lendi.")
        else:
            self.sonuc_ekrani.insert("end", "âš ï¸ AtÄ±f verisi bulunamadÄ±!\n")
            self.status_bar.configure(text="AtÄ±f verisi bulunamadÄ±.")

    def ilave_ozellikler_menusu(self):
        """
        Ä°lave Ã–zellikler MenÃ¼sÃ¼: Embedding Arama, KÃ¼meleme Analizi, Fine-Tuning Modeli, GeliÅŸmiÅŸ Veri Sorgulama.
        """
        # Embedding Arama
        self.embedding_btn = ctk.CTkButton(self, text="ğŸ” Embedding Arama", command=self._embedding_arama)
        self.embedding_btn.pack(pady=5)

        # KÃ¼meleme Analizi
        self.kumeleme_btn = ctk.CTkButton(self, text="ğŸ“Š KÃ¼meleme Analizi", command=self._kumeleme_analiz)
        self.kumeleme_btn.pack(pady=5)

        # Fine-Tuning Modeli
        self.fine_tune_btn = ctk.CTkButton(self, text="ğŸ‹â€â™‚ Fine-Tuning Modeli", command=self._fine_tune_model)
        self.fine_tune_btn.pack(pady=5)

        # GeliÅŸmiÅŸ Veri Sorgulama
        self.veri_sorgu_btn = ctk.CTkButton(self, text="ğŸ” GeliÅŸmiÅŸ Veri Sorgulama", command=self._veri_sorgu)
        self.veri_sorgu_btn.pack(pady=5)

    def _embedding_arama(self):
        """
        KullanÄ±cÄ±nÄ±n girdiÄŸi metinle en yakÄ±n embeddingleri arar.
        """
        query = self._kullanici_girdisi_al("Embedding Arama", "Aranacak metni girin:")
        if query:
            # search_embedding fonksiyonu, embedding modÃ¼lÃ¼nden veya alternatif embedding modÃ¼lÃ¼nden Ã§aÄŸrÄ±labilir.
            # Ã–rneÄŸin: search_embedding(query) ÅŸeklinde.
            try:
                from alternative_embedding_module import get_available_models, embed_text_with_model
                # Ã–rnek: ilk model ile dene, gerÃ§ek uygulamada daha geliÅŸmiÅŸ bir arama algoritmasÄ± kullanÄ±labilir.
                model_list = get_available_models()
                embedding = embed_text_with_model(query, model_list[0])
                result_text = f"Arama sonucu (Model: {model_list[0]}): {embedding[:10]} ... (embedding vektÃ¶rÃ¼)"
            except Exception as e:
                result_text = f"Embedding arama hatasÄ±: {e}"
            self._sonuc_goster("ğŸ” Embedding Arama SonuÃ§larÄ±", result_text)

    def _kumeleme_analiz(self):
        """
        KÃ¼meleme analizi yapar ve sonuÃ§larÄ± gÃ¶sterir.
        """
        try:
            clusters = perform_clustering()  # clustering_module.py'deki fonksiyon
            self._sonuc_goster("ğŸ“Š KÃ¼meleme Analizi SonuÃ§larÄ±", clusters)
        except Exception as e:
            self._sonuc_goster("ğŸ“Š KÃ¼meleme Analizi HatasÄ±", str(e))

    def _fine_tune_model(self):
        """
        Fine-Tuning Modeli eÄŸitimi baÅŸlatÄ±r ve sonuÃ§larÄ± gÃ¶sterir.
        """
        try:
            result = train_custom_model()  # fine_tuning_module.py'deki fonksiyon
            self._sonuc_goster("ğŸ‹â€â™‚ Fine-Tuning SonuÃ§larÄ±", result)
        except Exception as e:
            self._sonuc_goster("ğŸ‹â€â™‚ Fine-Tuning HatasÄ±", str(e))

    def _veri_sorgu(self):
        """
        GeliÅŸmiÅŸ veri sorgulama yapar.
        """
        query_params = self._kullanici_girdisi_al("Veri Sorgulama", "Sorgu parametrelerini girin:")
        if query_params:
            try:
                results = query_data(query_params)  # data_query_module.py'den fonksiyon
                self._sonuc_goster("ğŸ” Veri Sorgulama SonuÃ§larÄ±", results)
            except Exception as e:
                self._sonuc_goster("ğŸ” Veri Sorgulama HatasÄ±", str(e))

    def _sonuc_goster(self, baslik, icerik):
        """
        SonuÃ§larÄ± GUI Ã¼zerinde gÃ¶sterir.
        """
        self.sonuc_ekrani.insert("end", f"\n{baslik}:\n{icerik}\n")

    def _kullanici_girdisi_al(self, baslik, mesaj):
        """
        KullanÄ±cÄ±dan input almak iÃ§in diyalog penceresi aÃ§ar.
        """
        # CustomTkinter'Ä±n input diyalog kutusunu kullanÄ±yoruz.
        input_dialog = ctk.CTkInputDialog(title=baslik, text=mesaj)
        return input_dialog.get_input()

if __name__ == '__main__':
    # Ä°ÅŸlem yÃ¶neticisini oluÅŸtur ve GUI'yi baÅŸlat.
    islem_yoneticisi = IslemYoneticisi()
    arayuz = AnaArayuz(islem_yoneticisi)
    arayuz.mainloop()
    #---------------------------------------gui modulu sonu------------------------------------------------
    import os
import multiprocessing
from config_module import config
from processing_manager import IslemYoneticisi
from gui_module import AnaArayuz

if __name__ == '__main__':
    # Windows Ã¼zerinde Ã§oklu iÅŸlem desteÄŸi iÃ§in gerekli
    multiprocessing.freeze_support()
    
    try:
        # Ä°ÅŸlem yÃ¶neticisini oluÅŸturuyoruz
        islem_yoneticisi = IslemYoneticisi()
        
        # STORAGE_DIR iÃ§indeki dosya sayÄ±sÄ±nÄ± sayaÃ§lara aktaralÄ±m
        try:
            total_files = len(os.listdir(config.STORAGE_DIR))
        except Exception as e:
            config.logger.error(f"STORAGE_DIR okunamadÄ±: {e}")
            total_files = 0
        islem_yoneticisi.sayaÃ§lar['toplam'] = total_files
        
        # Ana GUI arayÃ¼zÃ¼nÃ¼ oluÅŸtur ve baÅŸlat
        arayuz = AnaArayuz(islem_yoneticisi)
        arayuz.mainloop()
    
    except Exception as e:
        config.logger.critical(f"Ana uygulama hatasÄ±: {e}", exc_info=True)
    
    finally:
        # Son istatistik raporu
        print("\nÄ°ÅŸlem TamamlandÄ±!")
        print(f"Toplam Dosya: {islem_yoneticisi.sayaÃ§lar.get('toplam', 0)}")
        print(f"BaÅŸarÄ±lÄ±: {islem_yoneticisi.sayaÃ§lar.get('baÅŸarÄ±lÄ±', 0)}")
        print(f"HatalÄ±: {islem_yoneticisi.sayaÃ§lar.get('hata', 0)}")

#---------------------------------------main modulu sonu------------------------------------------------
import os
import logging
import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from config_module import config

def perform_clustering(data, n_clusters=5, use_hdbscan=False):
    """
    ğŸ“Œ Verilen metin verileri Ã¼zerinde kÃ¼meleme analizi yapar.
    
    Ä°ÅŸ AkÄ±ÅŸÄ±:
      1. Girdi metinler, TfidfVectorizer kullanÄ±larak vektÃ¶rleÅŸtirilir (maksimum 1000 Ã¶zellik).
      2. EÄŸer use_hdbscan True ise HDBSCAN kullanÄ±larak kÃ¼meleme yapÄ±lmaya Ã§alÄ±ÅŸÄ±lÄ±r.
         - HDBSCAN kurulu deÄŸilse hata loglanÄ±r ve KMeans'e geÃ§ilir.
      3. VarsayÄ±lan olarak KMeans, n_clusters parametresi ile belirlenen kÃ¼me sayÄ±sÄ±na gÃ¶re kÃ¼meleme yapar.
      4. KÃ¼me etiketleri (labels) ve KMeans iÃ§in merkezler (centers) elde edilir.
      5. SonuÃ§lar, bir sÃ¶zlÃ¼k ÅŸeklinde (labels, centers, orijinal veriler) dÃ¶ndÃ¼rÃ¼lÃ¼r.
    
    Args:
        data (list): KÃ¼meleme yapÄ±lacak metin verilerinin listesi.
        n_clusters (int, optional): KMeans algoritmasÄ± iÃ§in kÃ¼me sayÄ±sÄ±. VarsayÄ±lan 5.
        use_hdbscan (bool, optional): True ise HDBSCAN algoritmasÄ± kullanÄ±lmaya Ã§alÄ±ÅŸÄ±lÄ±r. VarsayÄ±lan False.
    
    Returns:
        dict or None: KÃ¼meleme analizi sonucunu iÃ§eren sÃ¶zlÃ¼k ({"labels": ..., "centers": ..., "data": ...})
                      veya hata durumunda None.
    """
    if not data or not isinstance(data, list):
        config.logger.error("KÃ¼meleme iÃ§in geÃ§erli veri saÄŸlanamadÄ±.")
        return None

    try:
        # Metin verilerini TF-IDF vektÃ¶rlerine dÃ¶nÃ¼ÅŸtÃ¼r
        vectorizer = TfidfVectorizer(max_features=1000)
        X = vectorizer.fit_transform(data)
        config.logger.info("TF-IDF vektÃ¶rleÅŸtirme tamamlandÄ±.")
        
        if use_hdbscan:
            try:
                import hdbscan
                clusterer = hdbscan.HDBSCAN(min_cluster_size=2)
                labels = clusterer.fit_predict(X.toarray())
                centers = None  # HDBSCAN merkez bilgisi saÄŸlamaz
                config.logger.info("HDBSCAN ile kÃ¼meleme tamamlandÄ±.")
            except ImportError as ie:
                config.logger.error("HDBSCAN kÃ¼tÃ¼phanesi yÃ¼klÃ¼ deÄŸil, KMeans kullanÄ±lacak. (Hata: %s)", ie)
                use_hdbscan = False
        
        if not use_hdbscan:
            clusterer = KMeans(n_clusters=n_clusters, random_state=42)
            labels = clusterer.fit_predict(X)
            centers = clusterer.cluster_centers_.tolist()
            config.logger.info("KMeans ile kÃ¼meleme tamamlandÄ±.")
        
        result = {
            "labels": labels.tolist() if hasattr(labels, "tolist") else list(labels),
            "centers": centers,
            "data": data
        }
        return result

    except Exception as e:
        config.logger.error("KÃ¼meleme analizi sÄ±rasÄ±nda hata: %s", e, exc_info=True)
        return None

#---------------------------------------clustering_module sonu------------------------------------------------
import os
import json
import csv
import pandas as pd
from pathlib import Path
from config_module import config

def save_text_file(directory, filename, content):
    """
    Genel metin dosyalarÄ±nÄ± TXT formatÄ±nda kaydeder.
    
    Args:
        directory (str or Path): DosyanÄ±n kaydedileceÄŸi dizin.
        filename (str): Dosya adÄ± (uzantÄ± eklenmeyecek, fonksiyon ekleyecek).
        content (str): Kaydedilecek metin iÃ§eriÄŸi.
    
    Returns:
        str: OluÅŸturulan dosya yolunu dÃ¶ndÃ¼rÃ¼r.
    """
    directory = Path(directory)
    directory.mkdir(parents=True, exist_ok=True)
    file_path = directory / f"{filename}.txt"
    try:
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(content)
        config.logger.info(f"TXT dosyasÄ± kaydedildi: {file_path}")
        return str(file_path)
    except Exception as e:
        config.logger.error(f"TXT dosyasÄ± kaydedilemedi: {file_path}, Hata: {e}")
        return None

def save_json_file(directory, filename, content):
    """
    Veriyi JSON formatÄ±nda kaydeder.
    
    Args:
        directory (str or Path): DosyanÄ±n kaydedileceÄŸi dizin.
        filename (str): Dosya adÄ± (uzantÄ± eklenmeyecek).
        content (dict): Kaydedilecek veri.
    
    Returns:
        str: OluÅŸturulan dosya yolunu dÃ¶ndÃ¼rÃ¼r.
    """
    directory = Path(directory)
    directory.mkdir(parents=True, exist_ok=True)
    file_path = directory / f"{filename}.json"
    try:
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(content, f, ensure_ascii=False, indent=4)
        config.logger.info(f"JSON dosyasÄ± kaydedildi: {file_path}")
        return str(file_path)
    except Exception as e:
        config.logger.error(f"JSON dosyasÄ± kaydedilemedi: {file_path}, Hata: {e}")
        return None

def save_clean_text_files(original_filename, clean_text, bib_info):
    """
    Temiz metin dosyasÄ±nÄ± hem TXT hem de JSON formatÄ±nda, bibliyometrik bilgilerle birlikte kaydeder.
    
    Dosya isimlendirme: {ID}.clean.txt ve {ID}.clean.meta.json
    ID, dosyanÄ±n temel adÄ± (dokuman_id_al + shorten_title ile elde edilir).
    
    Args:
        original_filename (str): Ä°ÅŸlenen dosyanÄ±n orijinal adÄ±.
        clean_text (str): TemizlenmiÅŸ metin.
        bib_info (dict): Bibliyometrik bilgiler.
    
    Returns:
        dict: {"txt": dosya_yolu, "json": dosya_yolu} ÅŸeklinde dosya yollarÄ±.
    """
    base_name = Path(original_filename).stem
    txt_path = save_text_file(config.CLEAN_TEXT_DIR / "txt", f"{base_name}.clean", clean_text)
    json_path = save_json_file(config.CLEAN_TEXT_DIR / "json", f"{base_name}.clean.meta", bib_info)
    return {"txt": txt_path, "json": json_path}

def save_references_files(original_filename, references, bib_info):
    """
    KaynakÃ§a verilerini farklÄ± formatlarda kaydeder:
      - TXT: {ID}.references.txt
      - JSON: {ID}.references.meta.json
      - VOSviewer: {ID}.vos.references.txt
      - Pajek: {ID}.pjk.references.paj
      - CSV: {ID}.references.csv  (Her satÄ±rda bir kaynakÃ§a kaydÄ±)
    
    Args:
        original_filename (str): Orijinal dosya adÄ±.
        references (list): KaynakÃ§a metinlerinin listesi.
        bib_info (dict): Bibliyometrik bilgiler.
    
    Returns:
        dict: Kaydedilen dosya yollarÄ±nÄ± iÃ§eren sÃ¶zlÃ¼k.
    """
    base_name = Path(original_filename).stem
    ref_txt = save_text_file(config.REFERENCES_DIR / "txt", f"{base_name}.references", "\n".join(references))
    ref_json = save_json_file(config.REFERENCES_DIR / "json", f"{base_name}.references.meta", bib_info)
    # VOSviewer formatÄ±: Basit liste, her satÄ±rda bir kaynak
    vos_path = save_text_file(config.REFERENCES_DIR / "vosviewer", f"{base_name}.vos.references", "\n".join(references))
    # Pajek formatÄ±: Ä°lk satÄ±rda toplam vertex sayÄ±sÄ±, sonraki satÄ±rlarda id ve kaynakÃ§a metni
    pajek_file = config.REFERENCES_DIR / "pajek" / f"{base_name}.pjk.references.paj"
    (config.REFERENCES_DIR / "pajek").mkdir(parents=True, exist_ok=True)
    try:
        with open(pajek_file, 'w', encoding='utf-8') as f:
            f.write(f"*Vertices {len(references)}\n")
            for idx, ref in enumerate(references, start=1):
                f.write(f'{idx} "{ref}"\n')
        config.logger.info(f"Pajek dosyasÄ± kaydedildi: {pajek_file}")
        pajek_path = str(pajek_file)
    except Exception as e:
        config.logger.error(f"Pajek dosyasÄ± kaydedilemedi: {pajek_file}, Hata: {e}")
        pajek_path = None

    # CSV formatÄ±: Her kaynakÃ§a iÃ§in bir satÄ±r
    csv_file = config.REFERENCES_DIR / "csv" / f"{base_name}.references.csv"
    (config.REFERENCES_DIR / "csv").mkdir(parents=True, exist_ok=True)
    try:
        with open(csv_file, 'w', encoding='utf-8', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(["KaynakÃ§a"])
            for ref in references:
                writer.writerow([ref])
        config.logger.info(f"CSV dosyasÄ± kaydedildi: {csv_file}")
        csv_path = str(csv_file)
    except Exception as e:
        config.logger.error(f"CSV dosyasÄ± kaydedilemedi: {csv_file}, Hata: {e}")
        csv_path = None

    return {"txt": ref_txt, "json": ref_json, "vosviewer": vos_path, "pajek": pajek_path, "csv": csv_path}

def save_table_files(original_filename, table_data_list):
    """
    TablolarÄ± JSON, CSV ve Excel formatlarÄ±nda kaydeder.
    
    Args:
        original_filename (str): Orijinal dosya adÄ±.
        table_data_list (list): TablolarÄ±n verilerini iÃ§eren liste. Her tablo, 'baslik' ve 'veriler' anahtarlarÄ±na sahip sÃ¶zlÃ¼k olarak tanÄ±mlanmalÄ±.
    
    Returns:
        dict: Kaydedilen dosya yollarÄ±nÄ± iÃ§eren sÃ¶zlÃ¼k.
    """
    base_name = Path(original_filename).stem
    table_dir = config.TABLES_DIR
    table_dir.mkdir(parents=True, exist_ok=True)

    # JSON formatÄ±nda kaydet
    json_path = table_dir / f"{base_name}.tables.json"
    try:
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(table_data_list, f, ensure_ascii=False, indent=4)
        config.logger.info(f"Tablolar JSON formatÄ±nda kaydedildi: {json_path}")
    except Exception as e:
        config.logger.error(f"JSON dosyasÄ± kaydedilemedi: {json_path}, Hata: {e}")
    
    # CSV formatÄ±nda kaydet
    csv_path = table_dir / f"{base_name}.tables.csv"
    try:
        with open(csv_path, 'w', encoding='utf-8', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(["Tablo BaÅŸlÄ±ÄŸÄ±", "Tablo Ä°Ã§eriÄŸi"])
            for table in table_data_list:
                writer.writerow([table.get("baslik", ""), json.dumps(table.get("veriler", []), ensure_ascii=False)])
        config.logger.info(f"Tablolar CSV formatÄ±nda kaydedildi: {csv_path}")
    except Exception as e:
        config.logger.error(f"CSV dosyasÄ± kaydedilemedi: {csv_path}, Hata: {e}")

    # Excel formatÄ±nda kaydet
    excel_path = table_dir / f"{base_name}.tables.xlsx"
    try:
        writer = pd.ExcelWriter(excel_path, engine='xlsxwriter')
        for idx, table in enumerate(table_data_list, start=1):
            df = pd.DataFrame(table.get("veriler", []))
            sheet_name = f"Tablo{idx}"
            df.to_excel(writer, sheet_name=sheet_name, index=False)
        writer.save()
        config.logger.info(f"Tablolar Excel formatÄ±nda kaydedildi: {excel_path}")
    except Exception as e:
        config.logger.error(f"Excel dosyasÄ± kaydedilemedi: {excel_path}, Hata: {e}")

    return {"json": str(json_path), "csv": str(csv_path), "excel": str(excel_path)}

def save_embedding_file(original_filename, embedding_text, chunk_index):
    """
    Her dosyanÄ±n embedding verilerini kaydeder.
    Dosya isimlendirme: {ID}_chunk{chunk_index}.embed.txt
    
    Args:
        original_filename (str): Orijinal dosya adÄ±.
        embedding_text (str): Embedding verisinin kaydedilecek metin hali.
        chunk_index (int): Chunk numarasÄ±.
    
    Returns:
        str: OluÅŸturulan dosya yolunu dÃ¶ndÃ¼rÃ¼r.
    """
    base_name = Path(original_filename).stem
    embedding_dir = config.EMBEDDINGS_DIR
    embedding_dir.mkdir(parents=True, exist_ok=True)
    file_path = embedding_dir / f"{base_name}_chunk{chunk_index}.embed.txt"
    try:
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(embedding_text)
        config.logger.info(f"Embedding dosyasÄ± kaydedildi: {file_path}")
        return str(file_path)
    except Exception as e:
        config.logger.error(f"Embedding dosyasÄ± kaydedilemedi: {file_path}, Hata: {e}")
        return None

def save_chunked_text_files(original_filename, full_text, chunk_size=256):
    """
    BÃ¼yÃ¼k metni, belirlenen chunk boyutuna gÃ¶re bÃ¶lerek dosya sistemine kaydeder.
    Dosya isimlendirme: {ID}_part{parÃ§a_numarasÄ±}.txt
    
    Args:
        original_filename (str): Orijinal dosya adÄ±.
        full_text (str): TÃ¼m metin.
        chunk_size (int): Her chunk iÃ§in karakter sayÄ±sÄ± (varsayÄ±lan: 256).
    
    Returns:
        list: Kaydedilen tÃ¼m dosya yollarÄ±nÄ±n listesi.
    """
    base_name = Path(original_filename).stem
    chunk_dir = config.CLEAN_TEXT_DIR / "chunks"
    chunk_dir.mkdir(parents=True, exist_ok=True)
    text_chunks = [full_text[i:i + chunk_size] for i in range(0, len(full_text), chunk_size)]
    file_paths = []
    for idx, chunk in enumerate(text_chunks, start=1):
        file_path = save_text_file(chunk_dir, f"{base_name}_part{idx}", chunk)
        if file_path:
            file_paths.append(file_path)
    config.logger.info(f"BÃ¼yÃ¼k metin {len(text_chunks)} parÃ§aya bÃ¶lÃ¼ndÃ¼ ve kaydedildi.")
    return file_paths

#---------------------------------------fifine tuning modulu sonu------------------------------------------------

import os
from pathlib import Path
import glob
import json
import numpy as np
from config_module import config
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

def query_data(query_params):
    """
    ğŸ“Œ GeliÅŸmiÅŸ veri sorgulama fonksiyonu.
    
    Bu fonksiyon, config.CLEAN_TEXT_DIR / "txt" dizininde bulunan tÃ¼m temiz metin dosyalarÄ±nÄ±
    tarayarak, kullanÄ±cÄ± tarafÄ±ndan girilen sorgu parametrelerine gÃ¶re cosine similarity hesaplamasÄ± yapar.
    
    Ä°ÅŸ AkÄ±ÅŸÄ±:
      1. Belirtilen dizindeki tÃ¼m .txt dosyalarÄ± okunur.
      2. Her dosyadan elde edilen metinler, dosya adÄ± ile birlikte bir corpus oluÅŸturur.
      3. TfidfVectorizer kullanÄ±larak metinler vektÃ¶rleÅŸtirilir.
      4. KullanÄ±cÄ±nÄ±n sorgusu da aynÄ± ÅŸekilde vektÃ¶rleÅŸtirilir.
      5. Cosine similarity hesaplanarak en yÃ¼ksek skorlu sonuÃ§lar sÄ±ralanÄ±r.
      6. Her sonuÃ§ iÃ§in dosya adÄ±, benzerlik skoru ve ilk 200 karakterlik bir Ã¶zet (snippet) oluÅŸturulur.
    
    Args:
        query_params (str): KullanÄ±cÄ±nÄ±n sorgu olarak girdiÄŸi metin.
    
    Returns:
        dict: Sorgu sonuÃ§larÄ±nÄ± iÃ§eren sÃ¶zlÃ¼k. Ã–rnek:
              {
                  "results": [
                      {"file": "file1.txt", "similarity": 0.87, "snippet": "..." },
                      {"file": "file2.txt", "similarity": 0.75, "snippet": "..." },
                      ...
                  ]
              }
              Hata durumunda {"results": []} dÃ¶ndÃ¼rÃ¼lÃ¼r.
    """
    try:
        # Clean text dosyalarÄ±nÄ±n bulunduÄŸu dizin
        txt_dir = Path(config.CLEAN_TEXT_DIR) / "txt"
        if not txt_dir.exists():
            config.logger.error(f"Clean text dizini bulunamadÄ±: {txt_dir}")
            return {"results": []}
        
        # TÃ¼m .txt dosyalarÄ±nÄ± topla
        file_paths = list(txt_dir.glob("*.txt"))
        corpus = []
        file_names = []
        for file_path in file_paths:
            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    content = f.read()
                    corpus.append(content)
                    file_names.append(file_path.name)
            except Exception as e:
                config.logger.error(f"Dosya okunamadÄ±: {file_path} Hata: {e}")
        
        if not corpus:
            config.logger.error("Sorgulama iÃ§in temiz metin dosyalarÄ± bulunamadÄ±.")
            return {"results": []}
        
        # TF-IDF vektÃ¶rleÅŸtirme
        vectorizer = TfidfVectorizer(max_features=1000)
        X = vectorizer.fit_transform(corpus)
        
        # Sorgu metnini vektÃ¶rleÅŸtir
        query_vec = vectorizer.transform([query_params])
        
        # Cosine similarity hesapla
        similarities = cosine_similarity(X, query_vec).flatten()
        
        # Benzerlik skoruna gÃ¶re en yÃ¼ksek sonuÃ§larÄ± sÄ±rala
        top_indices = np.argsort(similarities)[::-1]
        
        results = []
        for idx in top_indices:
            sim_score = float(similarities[idx])
            if sim_score < 0.1:
                continue  # Ã‡ok dÃ¼ÅŸÃ¼k benzerlik, atla.
            snippet = corpus[idx][:200].replace("\n", " ")  # Ä°lk 200 karakter snippet olarak alÄ±nÄ±r
            results.append({
                "file": file_names[idx],
                "similarity": round(sim_score, 4),
                "snippet": snippet
            })
        
        config.logger.info(f"Veri sorgulama tamamlandÄ±, {len(results)} sonuÃ§ bulundu.")
        return {"results": results}
    except Exception as e:
        config.logger.error(f"Veri sorgulama sÄ±rasÄ±nda hata: {e}", exc_info=True)
        return {"results": []}
    
    #---------------------------------------data_query_module sonu------------------------------------------------
    